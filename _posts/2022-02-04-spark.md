---
title: "spark"
date: 2022-02-04T16:00:00-00:00
categories:
  - blog
tags:
  - spark
  - hadoop
  - hdfs
  - scala
  - pyspark
---

hadoop offers:
* hdfs: for storage
* mapreduce: for computation
* yarn: for resource management

spark is general purpose in memory compute engine, so its an alternative to mapreduce in hadoop, but would require storage (local/cloud) and resource manager(yarn/mesos/k8s).

spark can run on top of hadoop by replacing mapreduce with spark

### why spark

problem with mapreduce, it has to read and write from storage every time for each mapreduce job. And spark does all the processing in memory, so ideally just initial one read and final write to disk would be required. it is approximately 10-100% faster than hadoop.


### what do we mean by spark is general purpose

in hadoop we use:
* sqoop: for import
* pig: for cleaning
* hive: for querying
* mahout: for ML
* storm: for streaming

whereas in spark we can do all like cleaning, querying, ML, ingestion, streaming.


### storage unit in spark (rdd)

basic unit which holds data in spark is rdd (resilient distributed dataset). It is in memory distributed collection

```
//transfornations (lazy), nothing is executed only dag is being formed
rdd1 = load file1 from hdfs
rdd2 = rdd1.map

//action, it will be executed along with required transormations
rdd2.collect
```

lets say we have 500MB file in hdfs, then default we have 4 blocks data, based on 128MB block size. So, these 4 blocks will be loaded in memory as rdd in 4 partitions.

**resilient**: can recover from failure, if we loose rdd2 then through lineage graph its known how the rdd was created from its parent rdd. So, rdd provide fault tolerance through lineage graph. whereas in haddop resiliency is maintened by replication in hdfs.

**lineage graph**: keeps track of transormations to be executed after action has been called.

**immutable**: after rdd load data cannot be changed

### why immutable

because if we keep only one rdd and modify the same again and again, then in case of rdd failure at some step then we wont have parent rdd to regenrate then would require to do all the transformation from start.

### why lazy transformations

so as optimized plan can be made.

example: in below case we have to load all data then show one line only, it is better to optimize the plan knowing what all transormations are needed.
```
rdd1 = load data
rdd1 take 1 record to print
```

### frequency of word in a file

assume file is already present in some hdfs location `/path/to/file/file1`

running code interactively:
* scala: spark-shell
* python: pyspark

**spark context**: entrypoint to spark cluster, when we run interactively we can do `spark-shell`then it will return `sc`

```scala
//in scala
val rdd1 = sc.textFile("/path/to/file/file1")
val rdd2 = rdd1.flatMap(x => x.split(" ")) 
//flatMap takes each line as input 
//and will split based on " " in Array
//finally it will return only one Array with all the words
val rdd3 = rdd2.map(x => (x,1)) //each word will be key and 1 as value
val rdd4 = rdd3.reduceByKey((x,y) = x+y) //this is also transformation
//reduceByKey will have all the same keys together and takes 2 rows at a time 
//and operation we do is add so it will add values of the two rows
rdd4.collect()
```

```py
#in pyspark
sc.setLogLevel("ERROR") #default log is WARN
rdd1 = sc.textFile("/path/to/file/file1")
rdd2 = rdd1.flatMap(lambda x : x.split(" ")) 
rdd3 = rdd2.map(lambda x : (x,1))
rdd4 = rdd3.reduceByKey(lambda x,y : x+y)
rdd4.collect()
rdd4.saveAsTextFile("/path/to/file/output1")
```

**Note**
* at any point we can use `rdd.collect()` to see output of rdd
* in scala if there are no input parametrs to function then we can call without parentheses
* anonymous functions are called lambda in python
* to access local file we can use `file://` before the file path.
like `sc.textFile("file:///path/to/file/file1")`
* for doing lowercase we can use `toLowerCase()`
* for sorting we can use `sortby(x => x._2)` to sort on value in key value input
* if we need to just count number of times key is repeating then can use `countByValue` but its an action not transformation
* dag in pyspark is different than of scala because pyspark uses api library, scala dag matches to the code but not pyspark code
* at raw rdd level code pyspark is not that optimized compared to scala
* dag is created while transformations are being called and submitted when action is called
* lineage keeps track of rdds and how they are connected so as in case of failure, recovery can be done using parent rdd or if base rdd then getting data from specefic partition in disk, lineage is part of dag


### shared variables

#### broadcast

broadcast join in spark is similar to map side join in hive. It is aceived by using broadcast variable. Small table can be broadcasted to all nodes.

Example: If we need to filter out some data based on data in other small table then we could broadcast small table to all nodes. Lets say we have only one column in small table we could use `set` to have distinct values in a variable.

```scala
import scala.io.Source

var v:Set[String] = Set()
/*
//data to be broadcasted rows like: 
are
is
am
*/
val lines = Source.fromFile("/path/to/broadcast/data").getLines()
for(line <- lines) {v += line}

var b = sc.broadcast(v)

/*
//some data which has rows like:
hello how are you, 20
am good, 30
*/
val rdd1 = sc.textFile("/path/to/data")
val mappedInput = rdd1.map(x => x.split(",")(1).toFloat,x.split(",")(0))

//it will flat the structure based on values and produce more rows
val words = mappedInput.flatMapValues(x => x.split(" "))

//this will be map side work only to filter out data
val filterData = words.filer(x => !b.value(x._1))

//this will add values for same words
val total = filterData.reduceByKey((x,y) => x+y)

//this will give data in sorted desc
val sorted = total.sortBy(x => x._2,false)
```

#### accumlator

similar to counters in mapreduce, if we need to count then we can use accumlator.

* shared variable on driver machine
* each executor can only update it but cannot read value


```scala
val v = sc.longAccumlator("shown name in UI")
val rdd1 = sc.textFile("/path/to/data")

//it will count the number of lines in file
rdd1.foreach(x => v.add(1))
```
