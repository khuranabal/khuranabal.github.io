---
title: "yarn"
date: 2022-02-19T14:00:00-00:00
categories:
  - blog
tags:
  - hadoop
  - hdfs
  - yarn
---

Yet Another Resource Negotiator
Lets first go through how things are in hadoop initial version and what the limitations are which is solved by YARN.

### processing in hadoop / mr version 1

hdfs storage

**Name Node**: holds the metadata
**Data Node**: actual data in blocks

job execution is controlled by two processes

**master**: job tracker
**slave**: task tracker

#### job tracker

a lot of work is done in terms of-

**scheduling**: 

* deciding which job to execute first based on scheduling logic
* priority of jobs
* checking available resources
* allocating resources to jobs

**monitoring**:

* tracking the progress of job
* rerun task to recover from failure
* for slow task start on other node

#### task tracker

tracks tasks on each data node and informs the job tracker


#### limitations of hadoop / mr version 1

**scalability**: when cluster size goes beyond 4k data nodes then the job tracker used to become a bottleneck

**resource utilization**: need to configure fixed number of map and reduce slots at the time of cluster setup. if a job requires more map slots to be used then its problem

**job support**: only map reduce jobs were supported


### yarn

major bottleneck was that job tracker was doing a lot of work in scheduling and monitoring.

it has three components:

**resource manager**

* monitoring aspect is taken away from job tracker and in yarn its scalled as resource manager
* only does scheduling
* the resource manager creates a container(cpu + memory) on one of the node manager(task tracker), inside this container the resource manager creates an application master
* allocates the resources in the form of containers and details(containerid, hostname) will be send to application master

**node manager**

same as task tracker

**application master**

* take care of end to end monitoring for the application
* asks the resource manager for resources
* requests for resources in the form of number of containers(cpu + memory)

#### limitations of hadoop / mr version 1 (solved)

**scalability**: monitoring is offloaded to application master so for different applications, will have different application master to monitor

**resource utilization**: no more fixed map reduce slots, with the logical containers the resource allocation is much more dynamic and we can request for any amount of cpu or memory with this the cluster utilization is improved.

**job support**: can run spark, tez etc

#### uberization

if job is very small that can be solved inside application master itself then application master will not ask for more resources and run within, this mode is uberization.
