<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-82G1S3103X"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-82G1S3103X');
</script>

<!-- begin _includes/seo.html --><title>spark part-II - Balpreet Singh</title>
<meta name="description" content="spark core works on rdds (spark 1 style) but we have high level constructs to query/process data easily, its dataframe/datasets">



<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Balpreet Singh">
<meta property="og:title" content="spark part-II">
<meta property="og:url" content="http://localhost:4000/blog/spark-part-II/">


  <meta property="og:description" content="spark core works on rdds (spark 1 style) but we have high level constructs to query/process data easily, its dataframe/datasets">







  <meta property="article:published_time" content="2022-03-09T17:00:00+01:00">






<link rel="canonical" href="http://localhost:4000/blog/spark-part-II/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": null,
      "url": "http://localhost:4000/"
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Balpreet Singh Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Balpreet Singh
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/posts/">Posts</a>
            </li><li class="masthead__menu-item">
              <a href="/categories/">Categories</a>
            </li><li class="masthead__menu-item">
              <a href="/tags/">Tags</a>
            </li><li class="masthead__menu-item">
              <a href="/about/">About</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  
    <div class="author__avatar">
      
        <img src="/assets/images/bio-photo.jpg" alt="" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name"></h3>
    
    
      <div class="author__bio" itemprop="description">
        <p>Senior Data Engineer</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      

      
        
          
            <li><a href="https://khuranabal.github.io/" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-link" aria-hidden="true"></i><span class="label">Website</span></a></li>
          
        
          
            <li><a href="https://www.linkedin.com/in/balpreet-singh-654705114" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span class="label">Linkedin</span></a></li>
          
        
          
            <li><a href="https://github.com/khuranabal?tab=repositories" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="spark part-II">
    <meta itemprop="description" content="spark core works on rdds (spark 1 style) but we have high level constructs to query/process data easily, its dataframe/datasets">
    <meta itemprop="datePublished" content="2022-03-09T17:00:00+01:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">spark part-II
</h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          8 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
        <p>spark core works on rdds (spark 1 style) but we have high level constructs to query/process data easily, its dataframe/datasets</p>

<p>dataframe is distributed collection of data organized into named columns. It was available in spark 1 also but in spark 2 and onwards we have better support for dataframe/datasets and both are merged into single api (datset api)</p>

<p>dataframe code will be converted to low level rdd code which is done by driver. low level/rdd code is directly sent to executors
dataframe/dataset code optimized and converted to low level before sending to executor, using catalyst optimizer</p>

<h3 id="spark-session">spark session</h3>

<p>earlier we use to do spark conext and if hive required then hive context will be required, etc. spark session is unified and have all the contexts, kind of unified entry point for spark application. It is singleton object as we either get or create it.</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">org.apache.spark.sql.SparkSession</span>

<span class="k">val</span> <span class="nv">spark</span> <span class="k">=</span> <span class="nv">SparkSession</span><span class="o">.</span><span class="py">builder</span><span class="o">().</span><span class="py">appName</span><span class="o">(</span><span class="s">"app1"</span><span class="o">).</span><span class="py">master</span><span class="o">(</span><span class="s">"local[2]"</span><span class="o">).</span><span class="py">getOrCreate</span><span class="o">()</span>

<span class="c1">//do processing</span>

<span class="nv">spark</span><span class="o">.</span><span class="py">stop</span><span class="o">()</span>
</code></pre></div></div>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">//another way to set config</span>
<span class="k">import</span> <span class="nn">org.apache.spark.sql.SparkSession</span>
<span class="k">import</span> <span class="nn">org.apache.spark.SparkConf</span>

<span class="k">val</span> <span class="nv">sparkConf</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SparkConf</span><span class="o">()</span>
<span class="nv">sparkConf</span><span class="o">.</span><span class="py">set</span><span class="o">(</span><span class="s">"spark.app.name"</span><span class="o">,</span> <span class="s">"app1"</span><span class="o">)</span>
<span class="nv">sparkConf</span><span class="o">.</span><span class="py">set</span><span class="o">(</span><span class="s">"spark.master"</span><span class="o">,</span> <span class="s">"local[2]"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">spark</span> <span class="k">=</span> <span class="nv">SparkSession</span><span class="o">.</span><span class="py">builder</span><span class="o">().</span><span class="py">config</span><span class="o">(</span><span class="n">sparkConf</span><span class="o">).</span><span class="py">getOrCreate</span><span class="o">()</span>

<span class="c1">//do processing</span>

<span class="nv">spark</span><span class="o">.</span><span class="py">stop</span><span class="o">()</span>
</code></pre></div></div>

<h3 id="basics-dataframe-commands">basics dataframe commands</h3>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">import</span> <span class="nn">org.apache.log4j.Level</span>
<span class="k">import</span> <span class="nn">org.apache.log4j.Logger</span>

<span class="c1">//to get only ERROR and above</span>
<span class="nv">Logger</span><span class="o">.</span><span class="py">getLogger</span><span class="o">(</span><span class="s">"org"</span><span class="o">).</span><span class="py">setLevel</span><span class="o">(</span><span class="nv">level</span><span class="o">.</span><span class="py">ERROR</span><span class="o">)</span>

<span class="c1">//we should not use inferSchema in production as it infers data type based on few initial rows</span>
<span class="c1">//in spark it will create 3 jobs: 1st read, 2nd inferSchema as some data read, 3rd for show</span>
<span class="k">val</span> <span class="nv">df</span> <span class="k">=</span> <span class="nv">spark</span><span class="o">.</span><span class="py">read</span><span class="o">.</span><span class="py">option</span><span class="o">(</span><span class="s">"header"</span><span class="o">,</span> <span class="kc">true</span><span class="o">).</span><span class="py">option</span><span class="o">(</span><span class="s">"inferSchema"</span><span class="o">,</span> <span class="kc">true</span><span class="o">).</span><span class="py">csv</span><span class="o">(</span><span class="s">"/path"</span><span class="o">)</span>
<span class="nv">df</span><span class="o">.</span><span class="py">show</span><span class="o">()</span>
<span class="nv">df</span><span class="o">.</span><span class="py">printSchema</span><span class="o">()</span>

<span class="c1">//this will create 3 stages: 1st default, 2nd repartition, 3rd groupBy</span>
<span class="nv">df</span><span class="o">.</span><span class="py">repartition</span><span class="o">(</span><span class="mi">4</span><span class="o">).</span><span class="py">where</span><span class="o">(</span><span class="s">"col1&gt;10"</span><span class="o">).</span><span class="py">select</span><span class="o">(</span><span class="s">"col2,col3"</span><span class="o">).</span><span class="py">count</span><span class="o">().</span><span class="py">show</span><span class="o">()</span>

</code></pre></div></div>

<p><strong>Note</strong>: whenever shuffling is done then stage writes to Exchange (buffer) and next stage reads from Exchange. So we see in spark ui some data (compressed) written out and read in in next stage.</p>

<h3 id="rdddataframedataset">rdd/dataframe/dataset</h3>

<h4 id="rdd">rdd</h4>

<ul>
  <li>example map, filter, reduceByKey, etc.</li>
  <li>low level, not developer friendly</li>
  <li>lacks basic optimizations</li>
</ul>

<h4 id="dataframe">dataframe</h4>

<ul>
  <li>spark 1.3 and onwards</li>
  <li>high level construct, developer friendly</li>
</ul>

<p>challenges with dataframe:</p>

<ul>
  <li>not strongly typed, errors at runtime</li>
  <li>less flexibility</li>
</ul>

<p>dataframes can be converted to rdd to get flexibility and type safety, but the conversion has some cost involved and also rdd don’t go through catalyst optimizer, so major optimizations will be skipped if we work with rdd.</p>

<h4 id="dataset">dataset</h4>

<ul>
  <li>spark 1.6 and onwards</li>
  <li>compile time safety</li>
  <li>more flexibility to write low level code like anonymous/lambda functions</li>
  <li>conversion from dataframe to dataset is seemless</li>
</ul>

<p><strong>Note</strong>: before spark 2, dataframe and dataset had different api. In spark 2, both are merged into single unified structured api</p>

<p>So, dataframe is dataset of row type (dataset[row]), row is generic type which bounds at runtime. But in dataset type (dataset[Order]) is bound at compile time.</p>

<h4 id="convert-dataframe-to-dataset">convert dataframe to dataset</h4>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">case</span> <span class="k">class</span> <span class="nc">Abc</span><span class="o">(</span><span class="n">id</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span> <span class="n">col2</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">df</span> <span class="k">=</span> <span class="nv">spark</span><span class="o">.</span><span class="py">read</span><span class="o">.</span><span class="py">option</span><span class="o">(</span><span class="s">"header"</span><span class="o">,</span> <span class="kc">true</span><span class="o">).</span><span class="py">option</span><span class="o">(</span><span class="s">"inferSchema"</span><span class="o">,</span> <span class="kc">true</span><span class="o">).</span><span class="py">csv</span><span class="o">(</span><span class="s">"/path"</span><span class="o">)</span>

<span class="c1">//this is required for conversion from dataframe to dataset or vice versa</span>
<span class="c1">//it has to be imported after spark session only</span>
<span class="k">import</span> <span class="nn">spark.implicits._</span>

<span class="k">val</span> <span class="nv">ds</span> <span class="k">=</span> <span class="nv">df</span><span class="o">.</span><span class="py">as</span><span class="o">[</span><span class="kt">Abc</span><span class="o">]</span>

<span class="c1">//this will give error at complie time as col3 does not exist</span>
<span class="nv">ds</span><span class="o">.</span><span class="py">filter</span><span class="o">(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="nv">x</span><span class="o">.</span><span class="py">col3</span> <span class="o">&gt;</span> <span class="mi">3</span><span class="o">)</span>

<span class="c1">//we could use other way to filter but that will not give error at compile time</span>
<span class="nv">ds</span><span class="o">.</span><span class="py">filter</span><span class="o">(</span><span class="s">"col3 &gt; 3"</span><span class="o">)</span>
</code></pre></div></div>

<p><strong>Note</strong>: dataframes are preferred, as there is overhead in dataset for casting to particular type. with dataframe serializaton is managed by tungsten binary format, with dataset serialization is managed by java serialization which is slow</p>

<h3 id="read-modes">read modes</h3>

<p><strong>PERMISSIVE</strong>: sets NULL if encounters malformed record, this is default mode, _corrupt_record as new column will appear and it will hold the malformed record</p>

<p><strong>DROPMALFORMED</strong>: will ignore malformed record</p>

<p><strong>FAILFAST</strong>: exception raised when malformed record found</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">spark</span><span class="o">.</span><span class="py">read</span><span class="o">.</span><span class="py">format</span><span class="o">(</span><span class="s">"csv"</span><span class="o">).</span><span class="py">option</span><span class="o">(</span><span class="s">"header"</span><span class="o">,</span> <span class="kc">true</span><span class="o">).</span><span class="py">option</span><span class="o">(</span><span class="s">"path"</span><span class="o">,</span> <span class="s">"/path"</span><span class="o">).</span><span class="py">option</span><span class="o">(</span><span class="s">"mode"</span><span class="o">,</span> <span class="s">"FAILFAST"</span><span class="o">).</span><span class="py">load</span><span class="o">()</span>
</code></pre></div></div>

<h3 id="schema-types">schema types</h3>

<p><strong>INFER</strong>: as we do for csv infer schema</p>

<p><strong>IMPLICIT</strong>: example lile orc/parquet which comes with schema</p>

<p><strong>EXPLICIT</strong>: manually defining schema like using case class</p>

<h4 id="explicit-schema-approaches">explicit schema approaches</h4>

<p>programatic approach:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">//in this approach we give spark datatypes</span>
<span class="k">val</span> <span class="nv">dfSchema</span> <span class="k">=</span> <span class="nc">StructType</span><span class="o">(</span><span class="nc">List</span><span class="o">(</span><span class="nc">StructField</span><span class="o">(</span><span class="s">"col1"</span><span class="o">,</span> <span class="nc">IntegerType</span><span class="o">),</span><span class="nc">StructField</span><span class="o">(</span><span class="s">"col2"</span><span class="o">,</span> <span class="nc">TimestampType</span><span class="o">),</span><span class="nc">StructField</span><span class="o">(</span><span class="s">"col3"</span><span class="o">,</span> <span class="nc">IntegerType</span><span class="o">)))</span>

<span class="k">val</span> <span class="nv">df</span> <span class="k">=</span> <span class="nv">spark</span><span class="o">.</span><span class="py">read</span><span class="o">.</span><span class="py">format</span><span class="o">(</span><span class="s">"csv"</span><span class="o">).</span><span class="py">schema</span><span class="o">(</span><span class="n">dfSchema</span><span class="o">).</span><span class="py">option</span><span class="o">(</span><span class="s">"path"</span><span class="o">,</span> <span class="s">"/path"</span><span class="o">).</span><span class="py">load</span><span class="o">()</span>

<span class="nv">df</span><span class="o">.</span><span class="py">show</span><span class="o">()</span>
<span class="nv">df</span><span class="o">.</span><span class="py">printSchema</span><span class="o">()</span>
</code></pre></div></div>

<p>ddl string:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">//in this approach we give scala datatypes</span>
<span class="k">val</span> <span class="nv">dfSchema</span> <span class="k">=</span> <span class="s">"col1 Int, col2 Timestamp, col3 Int"</span>

<span class="k">val</span> <span class="nv">df</span> <span class="k">=</span> <span class="nv">spark</span><span class="o">.</span><span class="py">read</span><span class="o">.</span><span class="py">format</span><span class="o">(</span><span class="s">"csv"</span><span class="o">).</span><span class="py">schema</span><span class="o">(</span><span class="n">dfSchema</span><span class="o">).</span><span class="py">option</span><span class="o">(</span><span class="s">"path"</span><span class="o">,</span> <span class="s">"/path"</span><span class="o">).</span><span class="py">load</span><span class="o">()</span>

<span class="nv">df</span><span class="o">.</span><span class="py">show</span><span class="o">()</span>
<span class="nv">df</span><span class="o">.</span><span class="py">printSchema</span><span class="o">()</span>
</code></pre></div></div>

<h3 id="save-modes">save modes</h3>

<p><strong>append</strong>: add file in existing folder</p>

<p><strong>overwrite</strong>: overwrites the data in folder</p>

<p><strong>errorIfExists</strong>: error if folder exist</p>

<p><strong>ignore</strong>: ignore if folder exist</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">df</span><span class="o">.</span><span class="py">write</span><span class="o">.</span><span class="py">mode</span><span class="o">(</span><span class="nv">SaveMode</span><span class="o">.</span><span class="py">Overwrite</span><span class="o">).</span><span class="py">option</span><span class="o">(</span><span class="s">"path"</span><span class="o">,</span> <span class="s">"/path"</span><span class="o">).</span><span class="py">save</span><span class="o">()</span>
</code></pre></div></div>

<h4 id="spark-file-layout">spark file layout</h4>

<p>ways to control number of files generated when saving data, by default it will be equal to number of partitions in dataframe</p>

<p><strong>repartiton</strong>: it does full shuffle, not preferred</p>

<p><strong>partitioning/bucketing</strong>: partitoning will create folder and bucket is number of files</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">df</span><span class="o">.</span><span class="py">write</span><span class="o">.</span><span class="py">partitionBy</span><span class="o">(</span><span class="s">"col1"</span><span class="o">).</span><span class="py">mode</span><span class="o">(</span><span class="nv">SaveMode</span><span class="o">.</span><span class="py">Overwrite</span><span class="o">).</span><span class="py">option</span><span class="o">(</span><span class="s">"path"</span><span class="o">,</span> <span class="s">"/path"</span><span class="o">).</span><span class="py">save</span><span class="o">()</span>
</code></pre></div></div>

<p><strong>max records</strong>: if this option is set than each file will have that many records only</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">df</span><span class="o">.</span><span class="py">write</span><span class="o">.</span><span class="py">option</span><span class="o">(</span><span class="s">"maxRecordsPerFile"</span><span class="o">,</span> <span class="mi">1000</span><span class="o">).</span><span class="py">mode</span><span class="o">(</span><span class="nv">SaveMode</span><span class="o">.</span><span class="py">Overwrite</span><span class="o">).</span><span class="py">option</span><span class="o">(</span><span class="s">"path"</span><span class="o">,</span> <span class="s">"/path"</span><span class="o">).</span><span class="py">save</span><span class="o">()</span>
</code></pre></div></div>

<h3 id="sparksql">sparksql</h3>

<p>to work with sql we create view and work on it. example:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">//lets say we have dataframe already</span>

<span class="nv">df</span><span class="o">.</span><span class="py">createOrReplaceTempView</span><span class="o">(</span><span class="s">"dfView"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">output</span> <span class="k">=</span> <span class="nv">spak</span><span class="o">.</span><span class="py">sql</span><span class="o">(</span><span class="s">"select col1, max(col2) from dfView group by col1 order by col1"</span><span class="o">)</span>
<span class="nv">output</span><span class="o">.</span><span class="py">show</span><span class="o">()</span>
</code></pre></div></div>

<p><strong>Note</strong>: as per performance sparksql/dataframe are similar as both uses catalyst optimizer</p>

<h3 id="save-as-table">save as table</h3>

<p>when we use <code class="language-plaintext highlighter-rouge">save</code> then it saves directly as file in some folder. But sometimes we want to store as table (data + metadata)</p>

<p><strong>data</strong>: stored in path used in <code class="language-plaintext highlighter-rouge">spark.sql.warehouse.dir</code></p>

<p><strong>metadata</strong>: stored in memory by default, hive metstore can be used for permanent storage</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">//by default data will be stored in default location &amp; metadata in memory</span>
<span class="nv">df</span><span class="o">.</span><span class="py">write</span><span class="o">.</span><span class="py">format</span><span class="o">(</span><span class="s">"csv"</span><span class="o">).</span><span class="py">mode</span><span class="o">(</span><span class="nv">SaveMode</span><span class="o">.</span><span class="py">Overwrite</span><span class="o">).</span><span class="py">saveAsTable</span><span class="o">(</span><span class="s">"table1"</span><span class="o">)</span>
</code></pre></div></div>

<p><strong>Note</strong>:</p>
<ul>
  <li>we can enable hive support in config by <code class="language-plaintext highlighter-rouge">enableHiveStore()</code> to have permanent metadata store. So, we can create database and save tables in that database.</li>
  <li>save in table benefits if some query needs to be done for example by reporting tools etc.</li>
  <li>bucketing works on table, so here we can do like <code class="language-plaintext highlighter-rouge">bucketBy(2, "col1")</code>, this will create 2 files/buckets, it uses hash function so same data will end up in same bucket, it is widely used with <code class="language-plaintext highlighter-rouge">sortBy</code> for performance</li>
</ul>

<h3 id="transformations">transformations</h3>

<p><strong>low level</strong>: mostly used with rdds, some of it also works with dataframes/datasets</p>

<ul>
  <li>map</li>
  <li>filter</li>
  <li>groupByKey</li>
</ul>

<p><strong>high level</strong>: used with dataframes/datasets</p>

<ul>
  <li>select</li>
  <li>where</li>
  <li>groupBy</li>
</ul>

<p><strong>Note</strong>: although we can do work with dataframe/dataset but sometimes we need rdds for example if we have unstructured file with mutiple delimiters <code class="language-plaintext highlighter-rouge">1:data1,data2</code>. In this case we can read data in rdd and then process it using regex to get data and convert in dataset</p>

<h3 id="column-read">column read</h3>

<p><strong>column string</strong>: <code class="language-plaintext highlighter-rouge">df.select("col1","col2").show</code></p>

<p><strong>column object</strong>: <code class="language-plaintext highlighter-rouge">df.select(column("col1"),col("col2"),$"col3", 'col4).show</code></p>

<p><code class="language-plaintext highlighter-rouge">$"col3", 'col4</code> is supported only in scala, others are supported in both pyspark &amp; scala</p>

<p><strong>Note</strong>: both column string &amp; object can not be used in same statement</p>

<h3 id="column-expressions">column expressions</h3>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">df</span><span class="o">.</span><span class="py">select</span><span class="o">(</span><span class="nf">col</span><span class="o">(</span><span class="s">"col1"</span><span class="o">),</span> <span class="nf">expr</span><span class="o">(</span><span class="s">"concat(col2, '_suffix')"</span><span class="o">)).</span><span class="py">show</span><span class="o">()</span>

<span class="nv">df</span><span class="o">.</span><span class="py">selectExpr</span><span class="o">(</span><span class="s">"col1"</span><span class="o">,</span> <span class="s">"concat(col2, '_suffix')"</span><span class="o">).</span><span class="py">show</span><span class="o">()</span>
</code></pre></div></div>

<p><strong>Note</strong>: columns strings/object can not be used together with column expression</p>

<h3 id="attach-column-names-to-dataframe">attach column names to dataframe</h3>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">//lets say we have only data in the path without column names</span>
<span class="k">val</span> <span class="nv">df</span> <span class="k">=</span> <span class="nv">spark</span><span class="o">.</span><span class="py">read</span><span class="o">.</span><span class="py">format</span><span class="o">(</span><span class="s">"csv"</span><span class="o">).</span><span class="py">option</span><span class="o">(</span><span class="s">"path"</span><span class="o">,</span> <span class="s">"/path"</span><span class="o">).</span><span class="py">load</span><span class="o">()</span>

<span class="c1">//here column name are decided by spark but we can give name as below</span>
<span class="k">val</span> <span class="nv">df1</span> <span class="k">=</span> <span class="nv">df</span><span class="o">.</span><span class="py">toDF</span><span class="o">(</span><span class="s">"col1"</span><span class="o">,</span> <span class="s">"col2"</span><span class="o">,</span> <span class="s">"col3"</span><span class="o">)</span>

<span class="nv">df1</span><span class="o">.</span><span class="py">show</span><span class="o">()</span>
</code></pre></div></div>

<h3 id="udf">udf</h3>

<h4 id="column-object-expression-udf">column object expression udf</h4>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">//step1: define function</span>
<span class="k">def</span> <span class="nf">func</span><span class="o">(</span><span class="n">i</span><span class="k">:</span><span class="kt">Int</span><span class="o">)</span><span class="k">:</span><span class="kt">String</span> <span class="o">=</span> <span class="o">{</span>
  <span class="nf">if</span> <span class="o">(</span><span class="n">i</span><span class="o">&gt;</span><span class="mi">90</span><span class="o">)</span> <span class="s">"Y"</span> <span class="k">else</span> <span class="s">"N"</span>
<span class="o">}</span>

<span class="c1">//step2: register function</span>
<span class="k">val</span> <span class="nv">parseFunc</span> <span class="k">=</span> <span class="nf">udf</span><span class="o">(</span><span class="nf">func</span><span class="o">(</span><span class="k">_:</span><span class="kt">Int</span><span class="o">)</span><span class="k">:</span><span class="kt">String</span><span class="o">)</span>

<span class="c1">//read data</span>
<span class="k">val</span> <span class="nv">df</span> <span class="k">=</span> <span class="nv">spark</span><span class="o">.</span><span class="py">read</span><span class="o">.</span><span class="py">format</span><span class="o">(</span><span class="s">"csv"</span><span class="o">).</span><span class="py">option</span><span class="o">(</span><span class="s">"path"</span><span class="o">,</span> <span class="s">"/path"</span><span class="o">).</span><span class="py">load</span><span class="o">()</span>

<span class="c1">//step3: call function, driver will serialize function and send to all executors </span>
<span class="k">val</span> <span class="nv">df1</span><span class="k">=</span> <span class="nv">df</span><span class="o">.</span><span class="py">withColumn</span><span class="o">(</span><span class="s">"col4"</span><span class="o">,</span> <span class="nf">parseFunc</span><span class="o">(</span><span class="nf">col</span><span class="o">(</span><span class="s">"col1"</span><span class="o">)))</span>
<span class="nv">df1</span><span class="o">.</span><span class="py">show</span><span class="o">()</span>
</code></pre></div></div>

<h4 id="column-stringsql-udf">column string/sql udf</h4>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">//step1: define function</span>
<span class="k">def</span> <span class="nf">func</span><span class="o">(</span><span class="n">i</span><span class="k">:</span><span class="kt">Int</span><span class="o">)</span><span class="k">:</span><span class="kt">String</span> <span class="o">=</span> <span class="o">{</span>
  <span class="nf">if</span> <span class="o">(</span><span class="n">i</span><span class="o">&gt;</span><span class="mi">90</span><span class="o">)</span> <span class="s">"Y"</span> <span class="k">else</span> <span class="s">"N"</span>
<span class="o">}</span>

<span class="c1">//step2: register function, this will register in catalog and normal spark sql can also be done on it</span>
<span class="nv">spark</span><span class="o">.</span><span class="py">udf</span><span class="o">.</span><span class="py">register</span><span class="o">(</span><span class="s">"parseFunc"</span><span class="o">,</span><span class="nf">func</span><span class="o">(</span><span class="k">_:</span><span class="kt">Int</span><span class="o">)</span><span class="k">:</span><span class="kt">String</span><span class="o">)</span>

<span class="c1">//read data</span>
<span class="k">val</span> <span class="nv">df</span> <span class="k">=</span> <span class="nv">spark</span><span class="o">.</span><span class="py">read</span><span class="o">.</span><span class="py">format</span><span class="o">(</span><span class="s">"csv"</span><span class="o">).</span><span class="py">option</span><span class="o">(</span><span class="s">"path"</span><span class="o">,</span> <span class="s">"/path"</span><span class="o">).</span><span class="py">load</span><span class="o">()</span>

<span class="c1">//step3: call function, driver will serialize function and send to all executors </span>
<span class="k">val</span> <span class="nv">df1</span><span class="k">=</span> <span class="nv">df</span><span class="o">.</span><span class="py">withColumn</span><span class="o">(</span><span class="s">"col4"</span><span class="o">,</span> <span class="nf">expr</span><span class="o">(</span><span class="s">"parseFunc(col1)"</span><span class="o">))</span>
<span class="nv">df1</span><span class="o">.</span><span class="py">show</span><span class="o">()</span>
</code></pre></div></div>

<h3 id="example-with-test-data-in-code">example with test data in code</h3>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">//test data in list</span>
<span class="k">val</span> <span class="nv">l1</span> <span class="k">=</span> <span class="nc">List</span><span class="o">(</span>
  <span class="o">(</span><span class="mi">1</span><span class="o">,</span><span class="s">"2001-01-15"</span><span class="o">,</span><span class="mi">98</span><span class="o">,</span><span class="s">"PASS"</span><span class="o">),</span>
  <span class="o">(</span><span class="mi">2</span><span class="o">,</span><span class="s">"2001-01-15"</span><span class="o">,</span><span class="mi">9</span><span class="o">,</span><span class="s">"FAIL"</span><span class="o">),</span>
  <span class="o">(</span><span class="mi">3</span><span class="o">,</span><span class="s">"2001-01-15"</span><span class="o">,</span><span class="mi">60</span><span class="o">,</span><span class="s">"PASS"</span><span class="o">)</span>
<span class="o">)</span>

<span class="c1">//one way to get data in rdd than convert to df</span>
<span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">val</span> <span class="nv">rdd</span> <span class="k">=</span> <span class="nv">spark</span><span class="o">.</span><span class="py">sparkContext</span><span class="o">.</span><span class="py">parallelize</span><span class="o">(</span><span class="n">l1</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">df</span> <span class="k">=</span> <span class="nv">rdd</span><span class="o">.</span><span class="py">toDF</span><span class="o">()</span>

<span class="c1">//other way to directly get in df</span>
<span class="k">val</span> <span class="nv">df1</span> <span class="k">=</span> <span class="nv">spark</span><span class="o">.</span><span class="py">createDataFrame</span><span class="o">(</span><span class="n">l1</span><span class="o">).</span><span class="py">toDF</span><span class="o">(</span><span class="s">"col1"</span><span class="o">,</span><span class="s">"col2"</span><span class="o">,</span><span class="s">"col3"</span><span class="o">,</span><span class="s">"col4"</span><span class="o">)</span>

<span class="c1">//convert date column col2 (string) to epoch time</span>
<span class="k">val</span> <span class="nv">df2</span> <span class="k">=</span> <span class="nv">df1</span><span class="o">.</span><span class="py">withColumn</span><span class="o">(</span><span class="s">"col2"</span><span class="o">,</span> <span class="nf">unix_timestamp</span><span class="o">(</span><span class="nf">col</span><span class="o">(</span><span class="s">"col2"</span><span class="o">).</span><span class="py">cast</span><span class="o">(</span><span class="nc">DateType</span><span class="o">)))</span>

<span class="c1">//add new column with unique data</span>
<span class="k">val</span> <span class="nv">df3</span> <span class="k">=</span> <span class="nv">df2</span><span class="o">.</span><span class="py">withColumn</span><span class="o">(</span><span class="s">"new"</span><span class="o">,</span> <span class="n">monotonically_increasing_id</span><span class="o">)</span>

<span class="c1">//drop duplicates based on col1 and col2</span>
<span class="k">val</span> <span class="nv">df4</span> <span class="k">=</span> <span class="nv">df3</span><span class="o">.</span><span class="py">dropDuplicates</span><span class="o">(</span><span class="s">"col1"</span><span class="o">,</span><span class="s">"col2"</span><span class="o">)</span>

<span class="c1">//drop col3</span>
<span class="k">val</span> <span class="nv">df5</span> <span class="k">=</span> <span class="nv">df4</span><span class="o">.</span><span class="py">drop</span><span class="o">(</span><span class="s">"col3"</span><span class="o">)</span>
</code></pre></div></div>

<h3 id="aggregartions">aggregartions</h3>

<h4 id="simple">simple</h4>

<ul>
  <li>output is single row</li>
  <li>example sum/max/min of all data</li>
</ul>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">val</span> <span class="nv">df</span> <span class="k">=</span> <span class="nv">spark</span><span class="o">.</span><span class="py">read</span><span class="o">.</span><span class="py">format</span><span class="o">(</span><span class="s">"csv"</span><span class="o">).</span><span class="py">option</span><span class="o">(</span><span class="s">"path"</span><span class="o">,</span> <span class="s">"/path"</span><span class="o">).</span><span class="py">load</span><span class="o">()</span>

<span class="c1">//using column object expression</span>
<span class="nv">df</span><span class="o">.</span><span class="py">select</span><span class="o">(</span><span class="nf">count</span><span class="o">(</span><span class="s">"*"</span><span class="o">).</span><span class="py">as</span><span class="o">(</span><span class="s">"rows"</span><span class="o">),</span> <span class="nf">sum</span><span class="o">(</span><span class="s">"col3"</span><span class="o">).</span><span class="py">as</span><span class="o">(</span><span class="s">"total"</span><span class="o">),</span> <span class="nf">avg</span><span class="o">(</span><span class="s">"col3"</span><span class="o">).</span><span class="py">as</span><span class="o">(</span><span class="s">"avg"</span><span class="o">),</span> <span class="nf">countDistnct</span><span class="o">(</span><span class="s">"col4"</span><span class="o">).</span><span class="py">as</span><span class="o">(</span><span class="s">"distinct"</span><span class="o">)).</span><span class="py">show</span><span class="o">()</span>

<span class="c1">//using string expression</span>
<span class="nv">df</span><span class="o">.</span><span class="py">selectExpr</span><span class="o">(</span><span class="s">"count(*) as rows"</span><span class="o">,</span> <span class="s">"sum(col3) as total"</span><span class="o">,</span> <span class="s">"avg(col3) as avg"</span><span class="o">,</span> <span class="s">"countDistnct(col4) as distinct"</span><span class="o">).</span><span class="py">show</span><span class="o">()</span>

<span class="c1">//using spark sql</span>
<span class="nv">df</span><span class="o">.</span><span class="py">createOrReplaceTempView</span><span class="o">(</span><span class="s">"dfView"</span><span class="o">)</span>
<span class="nv">spark</span><span class="o">.</span><span class="py">sql</span><span class="o">(</span><span class="s">"select count(*) as rows, sum(col3) as total, avg(col3) as avg, countDistnct(col4) as distinct from dfView"</span><span class="o">).</span><span class="py">show</span><span class="o">()</span>
</code></pre></div></div>

<p><strong>Note</strong>: when count is done on particular column than only non null will be counted</p>

<h4 id="grouping">grouping</h4>

<ul>
  <li>output is more than one record</li>
  <li>groupBy is used</li>
</ul>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">val</span> <span class="nv">df</span> <span class="k">=</span> <span class="nv">spark</span><span class="o">.</span><span class="py">read</span><span class="o">.</span><span class="py">format</span><span class="o">(</span><span class="s">"csv"</span><span class="o">).</span><span class="py">option</span><span class="o">(</span><span class="s">"path"</span><span class="o">,</span> <span class="s">"/path"</span><span class="o">).</span><span class="py">load</span><span class="o">()</span>

<span class="nv">df</span><span class="o">.</span><span class="py">groupBy</span><span class="o">(</span><span class="s">"col1"</span><span class="o">,</span><span class="s">"col2"</span><span class="o">).</span><span class="py">agg</span><span class="o">(</span><span class="nf">sum</span><span class="o">(</span><span class="s">"col3"</span><span class="o">).</span><span class="py">as</span><span class="o">(</span><span class="s">"sum"</span><span class="o">),</span><span class="nf">sum</span><span class="o">(</span><span class="nf">expr</span><span class="o">(</span><span class="s">"col3*col3"</span><span class="o">).</span><span class="py">as</span><span class="o">(</span><span class="s">"square"</span><span class="o">))).</span><span class="py">show</span><span class="o">()</span>
</code></pre></div></div>

<h4 id="window">window</h4>

<ul>
  <li>some fixed size window is used</li>
  <li>example past 7 days sale</li>
</ul>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">val</span> <span class="nv">df</span> <span class="k">=</span> <span class="nv">spark</span><span class="o">.</span><span class="py">read</span><span class="o">.</span><span class="py">format</span><span class="o">(</span><span class="s">"csv"</span><span class="o">).</span><span class="py">option</span><span class="o">(</span><span class="s">"path"</span><span class="o">,</span> <span class="s">"/path"</span><span class="o">).</span><span class="py">load</span><span class="o">()</span>

<span class="c1">//window need partition, order, window size</span>
<span class="k">val</span> <span class="nv">window</span> <span class="k">=</span> <span class="nv">Window</span><span class="o">.</span><span class="py">partitionBy</span><span class="o">(</span><span class="s">"col1"</span><span class="o">).</span><span class="py">orderBy</span><span class="o">(</span><span class="s">"col2"</span><span class="o">).</span><span class="py">rowsBetween</span><span class="o">(</span><span class="nv">Window</span><span class="o">.</span><span class="py">unboundedPreceding</span><span class="o">,</span> <span class="nv">Windows</span><span class="o">.</span><span class="py">currentRow</span><span class="o">)</span>

<span class="nv">df</span><span class="o">.</span><span class="py">withColumn</span><span class="o">(</span><span class="s">"runningSum"</span><span class="o">,</span> <span class="nf">sum</span><span class="o">(</span><span class="s">"col3"</span><span class="o">).</span><span class="py">over</span><span class="o">(</span><span class="n">window</span><span class="o">)).</span><span class="py">show</span><span class="o">()</span>
</code></pre></div></div>

<h3 id="join">join</h3>

<h4 id="simple-1">simple</h4>

<p>these are the ones where shuffle is involved, also called as shuffle sort merge join</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">val</span> <span class="nv">df1</span> <span class="k">=</span> <span class="nv">spark</span><span class="o">.</span><span class="py">read</span><span class="o">.</span><span class="py">format</span><span class="o">(</span><span class="s">"csv"</span><span class="o">).</span><span class="py">option</span><span class="o">(</span><span class="s">"path"</span><span class="o">,</span> <span class="s">"/path1"</span><span class="o">).</span><span class="py">load</span>
<span class="k">val</span> <span class="nv">df2</span> <span class="k">=</span> <span class="nv">spark</span><span class="o">.</span><span class="py">read</span><span class="o">.</span><span class="py">format</span><span class="o">(</span><span class="s">"csv"</span><span class="o">).</span><span class="py">option</span><span class="o">(</span><span class="s">"path"</span><span class="o">,</span> <span class="s">"/path2"</span><span class="o">).</span><span class="py">load</span>

<span class="k">val</span> <span class="nv">joinCondition</span> <span class="k">=</span> <span class="nv">df1</span><span class="o">.</span><span class="py">col</span><span class="o">(</span><span class="s">"col1"</span><span class="o">)</span> <span class="o">===</span> <span class="nv">df2</span><span class="o">.</span><span class="py">col</span><span class="o">(</span><span class="s">"col1"</span><span class="o">)</span>
<span class="nv">df1</span><span class="o">.</span><span class="py">join</span><span class="o">(</span><span class="n">df2</span><span class="o">,</span> <span class="n">joinCondition</span><span class="o">,</span> <span class="s">"inner"</span><span class="o">).</span><span class="py">show</span><span class="o">()</span>
</code></pre></div></div>

<p><strong>type of joins</strong></p>

<ul>
  <li>inner: matching records</li>
  <li>left: matching + non matching from left</li>
  <li>right: matching + non matching from right</li>
  <li>outer: matching + non matching from left + non matching from right</li>
</ul>

<p><strong>internals of join</strong></p>

<ul>
  <li>for join to happen keys needs to be on same executor (reducer)</li>
  <li>write output to exchange, which is buffer in the executor (mapper)</li>
  <li>from exchange spark framework will read and do shuffle so as same keys goes to same executor</li>
</ul>

<p><strong>Note</strong>:</p>
<ul>
  <li>in join, if both dataframes have same column name then selecting that particular column will be ambiguous. either need to rename column in one of the dataframe <code class="language-plaintext highlighter-rouge">withColumnRenamed</code> before join or drop the column after join.</li>
  <li>in case of null, use <code class="language-plaintext highlighter-rouge">coalesce</code> to get some default value or something</li>
</ul>

<h4 id="broadcast">broadcast</h4>

<ul>
  <li>small tables is broadcasted to all the executors, no shuffle</li>
  <li>one table should be small enough so as all executors can hold data in memory</li>
  <li>by default its enabled to broadcast small files</li>
  <li>can be disabled by <code class="language-plaintext highlighter-rouge">spark.sql("SET spark.sql.autoBroadcastJoinThreshold = -1")</code></li>
  <li>can also be used as <code class="language-plaintext highlighter-rouge">df1.join(broadcast(df2), joinCondition, "inner")</code></li>
</ul>

<h3 id="pivot">pivot</h3>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">//lets say temp view dfView exist</span>
<span class="c1">//here pivot on col2 have to query to find distinct</span>
<span class="k">val</span> <span class="nv">df</span> <span class="k">=</span> <span class="nv">spark</span><span class="o">.</span><span class="py">sql</span><span class="o">(</span><span class="s">"select col1, col2 from dfView"</span><span class="o">).</span><span class="py">groupBy</span><span class="o">(</span><span class="s">"col1"</span><span class="o">).</span><span class="py">pivot</span><span class="o">(</span><span class="s">"col2"</span><span class="o">).</span><span class="py">count</span><span class="o">().</span><span class="py">show</span><span class="o">()</span>

<span class="c1">//we can fix the data in pivot so as it does not have to do distinct query</span>
<span class="k">val</span> <span class="nv">columns</span> <span class="k">=</span> <span class="nc">List</span><span class="o">(</span><span class="s">"Yes"</span><span class="o">,</span> <span class="s">"No"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">df</span> <span class="k">=</span> <span class="nv">spark</span><span class="o">.</span><span class="py">sql</span><span class="o">(</span><span class="s">"select col1, col2 from dfView"</span><span class="o">).</span><span class="py">groupBy</span><span class="o">(</span><span class="s">"col1"</span><span class="o">).</span><span class="py">pivot</span><span class="o">(</span><span class="s">"col2"</span><span class="o">,</span> <span class="n">columns</span><span class="o">).</span><span class="py">count</span><span class="o">().</span><span class="py">show</span><span class="o">()</span>
</code></pre></div></div>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#spark" class="page__taxonomy-item" rel="tag">spark</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#blog" class="page__taxonomy-item" rel="tag">blog</a>
    
    </span>
  </p>


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2022-03-09T17:00:00+01:00">March 9, 2022</time></p>


      </footer>

      

      
  <nav class="pagination">
    
      <a href="/blog/hive-basics/" class="pagination--pager" title="hive basics
">Previous</a>
    
    
      <a href="/blog/spark-optimizations/" class="pagination--pager" title="spark optimizations
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You May Also Enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/blog/spark-optimizations/" rel="permalink">spark optimizations
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          6 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">optimizations can be at application code level or at cluster level, here we are looking more at cluster level optimizations

</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/blog/hive-basics/" rel="permalink">hive basics
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          6 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">it is open source datawarehouse to process structured data on top of hadoop

</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/blog/yarn/" rel="permalink">yarn
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          3 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Yet Another Resource Negotiator
Lets first go through how things are in hadoop initial version and what the limitations are which is solved by YARN.

</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/blog/python/" rel="permalink">python basics
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          less than 1 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">module

</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://www.linkedin.com/in/balpreet-singh-654705114" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> Linkedin</a></li>
        
      
        
          <li><a href="https://github.com/khuranabal?tab=repositories" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2022 Balpreet Singh. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




    <script>
  'use strict';

  (function() {
    var commentContainer = document.querySelector('#utterances-comments');

    if (!commentContainer) {
      return;
    }

    var script = document.createElement('script');
    script.setAttribute('src', 'https://utteranc.es/client.js');
    script.setAttribute('repo', 'khuranabal/khuranabal.github.io');
    script.setAttribute('issue-term', 'pathname');
    script.setAttribute('theme', 'github-light');
    script.setAttribute('crossorigin', 'anonymous');

    commentContainer.appendChild(script);
  })();
</script>

  





  </body>
</html>
