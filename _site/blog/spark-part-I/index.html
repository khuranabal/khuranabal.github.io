<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-82G1S3103X"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-82G1S3103X');
</script>

<!-- begin _includes/seo.html --><title>spark part-I - Balpreet Singh</title>
<meta name="description" content="hadoop offers:    hdfs: for storage   mapreduce: for computation   yarn: for resource management">



<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Balpreet Singh">
<meta property="og:title" content="spark part-I">
<meta property="og:url" content="http://localhost:4000/blog/spark-part-I/">


  <meta property="og:description" content="hadoop offers:    hdfs: for storage   mapreduce: for computation   yarn: for resource management">







  <meta property="article:published_time" content="2022-02-04T17:00:00+01:00">






<link rel="canonical" href="http://localhost:4000/blog/spark-part-I/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": null,
      "url": "http://localhost:4000/"
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Balpreet Singh Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Balpreet Singh
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/posts/">Posts</a>
            </li><li class="masthead__menu-item">
              <a href="/categories/">Categories</a>
            </li><li class="masthead__menu-item">
              <a href="/tags/">Tags</a>
            </li><li class="masthead__menu-item">
              <a href="/about/">About</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  
    <div class="author__avatar">
      
        <img src="/assets/images/bio-photo.jpg" alt="" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name"></h3>
    
    
      <div class="author__bio" itemprop="description">
        <p>Senior Data Engineer</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      

      
        
          
            <li><a href="https://khuranabal.github.io/" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-link" aria-hidden="true"></i><span class="label">Website</span></a></li>
          
        
          
            <li><a href="https://www.linkedin.com/in/balpreet-singh-654705114" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span class="label">Linkedin</span></a></li>
          
        
          
            <li><a href="https://github.com/khuranabal?tab=repositories" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="spark part-I">
    <meta itemprop="description" content="hadoop offers:  hdfs: for storage  mapreduce: for computation  yarn: for resource management">
    <meta itemprop="datePublished" content="2022-02-04T17:00:00+01:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">spark part-I
</h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          10 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
        <p>hadoop offers:</p>
<ul>
  <li>hdfs: for storage</li>
  <li>mapreduce: for computation</li>
  <li>yarn: for resource management</li>
</ul>

<p>spark is general purpose in memory compute engine, so its an alternative to mapreduce in hadoop, but would require storage (local/cloud) and resource manager(yarn/mesos/k8s).</p>

<p>spark can run on top of hadoop by replacing mapreduce with spark</p>

<h3 id="why-spark">why spark</h3>

<p>problem with mapreduce, it has to read and write from storage every time for each mapreduce job. And spark does all the processing in memory, so ideally just initial one read and final write to disk would be required. it is approximately 10-100% faster than hadoop.</p>

<h3 id="what-do-we-mean-by-spark-is-general-purpose">what do we mean by spark is general purpose</h3>

<p>in hadoop we use:</p>
<ul>
  <li>sqoop: for import</li>
  <li>pig: for cleaning</li>
  <li>hive: for querying</li>
  <li>mahout: for ML</li>
  <li>storm: for streaming</li>
</ul>

<p>whereas in spark we can do all like cleaning, querying, ML, ingestion, streaming.</p>

<h3 id="storage-unit-in-spark-rdd">storage unit in spark (rdd)</h3>

<p>basic unit which holds data in spark is rdd (resilient distributed dataset). It is in memory distributed collection</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>//transfornations (lazy), nothing is executed only dag is being formed
rdd1 = load file1 from hdfs
rdd2 = rdd1.map

//action, it will be executed along with required transormations
rdd2.collect
</code></pre></div></div>

<p>lets say we have 500MB file in hdfs, then default we have 4 blocks data, based on 128MB block size. So, these 4 blocks will be loaded in memory as rdd in 4 partitions.</p>

<p><strong>resilient</strong>: can recover from failure, if we loose rdd2 then through lineage graph its known how the rdd was created from its parent rdd. So, rdd provide fault tolerance through lineage graph. whereas in haddop resiliency is maintened by replication in hdfs.</p>

<p><strong>lineage graph</strong>: keeps track of transormations to be executed after action has been called.</p>

<p><strong>immutable</strong>: after rdd load data cannot be changed</p>

<h3 id="why-immutable">why immutable</h3>

<p>because if we keep only one rdd and modify the same again and again, then in case of rdd failure at some step then we wont have parent rdd to regenrate then would require to do all the transformation from start.</p>

<h3 id="why-lazy-transformations">why lazy transformations</h3>

<p>so as optimized plan can be made.</p>

<p>example: in below case we have to load all data then show one line only, it is better to optimize the plan knowing what all transormations are needed.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>rdd1 = load data
rdd1 take 1 record to print
</code></pre></div></div>

<h3 id="frequency-of-word-in-a-file">frequency of word in a file</h3>

<p>assume file is already present in some hdfs location <code class="language-plaintext highlighter-rouge">/path/to/file/file1</code></p>

<p>running code interactively:</p>
<ul>
  <li>scala: spark-shell</li>
  <li>python: pyspark</li>
</ul>

<p><strong>spark context</strong>: entrypoint to spark cluster, when we run interactively we can do <code class="language-plaintext highlighter-rouge">spark-shell</code>then it will return <code class="language-plaintext highlighter-rouge">sc</code></p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">//in scala</span>
<span class="k">val</span> <span class="nv">rdd1</span> <span class="k">=</span> <span class="nv">sc</span><span class="o">.</span><span class="py">textFile</span><span class="o">(</span><span class="s">"/path/to/file/file1"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">rdd2</span> <span class="k">=</span> <span class="nv">rdd1</span><span class="o">.</span><span class="py">flatMap</span><span class="o">(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="nv">x</span><span class="o">.</span><span class="py">split</span><span class="o">(</span><span class="s">" "</span><span class="o">))</span> 
<span class="c1">//flatMap takes each line as input </span>
<span class="c1">//and will split based on " " in Array</span>
<span class="c1">//finally it will return only one Array with all the words</span>
<span class="k">val</span> <span class="nv">rdd3</span> <span class="k">=</span> <span class="nv">rdd2</span><span class="o">.</span><span class="py">map</span><span class="o">(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">x</span><span class="o">,</span><span class="mi">1</span><span class="o">))</span> <span class="c1">//each word will be key and 1 as value</span>
<span class="k">val</span> <span class="nv">rdd4</span> <span class="k">=</span> <span class="nv">rdd3</span><span class="o">.</span><span class="py">reduceByKey</span><span class="o">((</span><span class="n">x</span><span class="o">,</span><span class="n">y</span><span class="o">)</span> <span class="k">=</span> <span class="n">x</span><span class="o">+</span><span class="n">y</span><span class="o">)</span> <span class="c1">//this is also transformation</span>
<span class="c1">//reduceByKey will have all the same keys together and takes 2 rows at a time </span>
<span class="c1">//and operation we do is add so it will add values of the two rows</span>
<span class="nv">rdd4</span><span class="o">.</span><span class="py">collect</span><span class="o">()</span>
</code></pre></div></div>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#in pyspark
</span><span class="n">sc</span><span class="p">.</span><span class="n">setLogLevel</span><span class="p">(</span><span class="s">"ERROR"</span><span class="p">)</span> <span class="c1">#default log is WARN
</span><span class="n">rdd1</span> <span class="o">=</span> <span class="n">sc</span><span class="p">.</span><span class="n">textFile</span><span class="p">(</span><span class="s">"/path/to/file/file1"</span><span class="p">)</span>
<span class="n">rdd2</span> <span class="o">=</span> <span class="n">rdd1</span><span class="p">.</span><span class="n">flatMap</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">x</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="s">" "</span><span class="p">))</span> 
<span class="n">rdd3</span> <span class="o">=</span> <span class="n">rdd2</span><span class="p">.</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">rdd4</span> <span class="o">=</span> <span class="n">rdd3</span><span class="p">.</span><span class="n">reduceByKey</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="p">:</span> <span class="n">x</span><span class="o">+</span><span class="n">y</span><span class="p">)</span>
<span class="n">rdd4</span><span class="p">.</span><span class="n">collect</span><span class="p">()</span>
<span class="n">rdd4</span><span class="p">.</span><span class="n">saveAsTextFile</span><span class="p">(</span><span class="s">"/path/to/file/output1"</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Note</strong></p>
<ul>
  <li>at any point we can use <code class="language-plaintext highlighter-rouge">rdd.collect()</code> to see output of rdd</li>
  <li>in scala if there are no input parametrs to function then we can call without parentheses</li>
  <li>anonymous functions are called lambda in python</li>
  <li>to access local file we can use <code class="language-plaintext highlighter-rouge">file://</code> before the file path.
like <code class="language-plaintext highlighter-rouge">sc.textFile("file:///path/to/file/file1")</code></li>
  <li>for doing lowercase we can use <code class="language-plaintext highlighter-rouge">toLowerCase()</code></li>
  <li>for sorting we can use <code class="language-plaintext highlighter-rouge">sortby(x =&gt; x._2)</code> to sort on value in key value input</li>
  <li>if we need to just count number of times key is repeating then can use <code class="language-plaintext highlighter-rouge">countByValue</code> but its an action not transformation</li>
  <li>dag in pyspark is different than of scala because pyspark uses api library, scala dag matches to the code but not pyspark code</li>
  <li>at raw rdd level code pyspark is not that optimized compared to scala</li>
  <li>dag is created while transformations are being called and submitted when action is called, it shows jobs, stages, tasks</li>
  <li>lineage keeps track of rdds and how they are connected so as in case of failure, recovery can be done using parent rdd or if base rdd then getting data from specefic partition in disk, lineage is part of dag</li>
  <li>we can check lineage on any rdd with function <code class="language-plaintext highlighter-rouge">toDebugString</code> like <code class="language-plaintext highlighter-rouge">rdd1.toDebugString</code></li>
</ul>

<h3 id="shared-variables">shared variables</h3>

<h4 id="broadcast">broadcast</h4>

<p>broadcast join in spark is similar to map side join in hive. It is aceived by using broadcast variable. Small table can be broadcasted to all nodes.</p>

<p>Example: If we need to filter out some data based on data in other small table then we could broadcast small table to all nodes. Lets say we have only one column in small table we could use <code class="language-plaintext highlighter-rouge">set</code> to have distinct values in a variable.</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">scala.io.Source</span>

<span class="k">var</span> <span class="n">v</span><span class="k">:</span><span class="kt">Set</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="nc">Set</span><span class="o">()</span>
<span class="cm">/*
//data to be broadcasted rows like: 
are
is
am
*/</span>
<span class="k">val</span> <span class="nv">lines</span> <span class="k">=</span> <span class="nv">Source</span><span class="o">.</span><span class="py">fromFile</span><span class="o">(</span><span class="s">"/path/to/broadcast/data"</span><span class="o">).</span><span class="py">getLines</span><span class="o">()</span>
<span class="nf">for</span><span class="o">(</span><span class="n">line</span> <span class="k">&lt;-</span> <span class="n">lines</span><span class="o">)</span> <span class="o">{</span><span class="n">v</span> <span class="o">+=</span> <span class="n">line</span><span class="o">}</span>

<span class="k">var</span> <span class="n">b</span> <span class="k">=</span> <span class="nv">sc</span><span class="o">.</span><span class="py">broadcast</span><span class="o">(</span><span class="n">v</span><span class="o">)</span>

<span class="cm">/*
//some data which has rows like:
hello how are you, 20
am good, 30
*/</span>
<span class="k">val</span> <span class="nv">rdd1</span> <span class="k">=</span> <span class="nv">sc</span><span class="o">.</span><span class="py">textFile</span><span class="o">(</span><span class="s">"/path/to/data"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">mappedInput</span> <span class="k">=</span> <span class="nv">rdd1</span><span class="o">.</span><span class="py">map</span><span class="o">(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="nv">x</span><span class="o">.</span><span class="py">split</span><span class="o">(</span><span class="s">","</span><span class="o">)(</span><span class="mi">1</span><span class="o">).</span><span class="py">toFloat</span><span class="o">,</span><span class="nv">x</span><span class="o">.</span><span class="py">split</span><span class="o">(</span><span class="s">","</span><span class="o">)(</span><span class="mi">0</span><span class="o">))</span>

<span class="c1">//it will flat the structure based on values and produce more rows</span>
<span class="k">val</span> <span class="nv">words</span> <span class="k">=</span> <span class="nv">mappedInput</span><span class="o">.</span><span class="py">flatMapValues</span><span class="o">(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="nv">x</span><span class="o">.</span><span class="py">split</span><span class="o">(</span><span class="s">" "</span><span class="o">))</span>

<span class="c1">//this will be map side work only to filter out data</span>
<span class="k">val</span> <span class="nv">filterData</span> <span class="k">=</span> <span class="nv">words</span><span class="o">.</span><span class="py">filer</span><span class="o">(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="o">!</span><span class="nv">b</span><span class="o">.</span><span class="py">value</span><span class="o">(</span><span class="nv">x</span><span class="o">.</span><span class="py">_1</span><span class="o">))</span>

<span class="c1">//this will add values for same words</span>
<span class="k">val</span> <span class="nv">total</span> <span class="k">=</span> <span class="nv">filterData</span><span class="o">.</span><span class="py">reduceByKey</span><span class="o">((</span><span class="n">x</span><span class="o">,</span><span class="n">y</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">x</span><span class="o">+</span><span class="n">y</span><span class="o">)</span>

<span class="c1">//this will give data in sorted desc</span>
<span class="k">val</span> <span class="nv">sorted</span> <span class="k">=</span> <span class="nv">total</span><span class="o">.</span><span class="py">sortBy</span><span class="o">(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="nv">x</span><span class="o">.</span><span class="py">_2</span><span class="o">,</span><span class="kc">false</span><span class="o">)</span>
</code></pre></div></div>

<h4 id="accumlator">accumlator</h4>

<p>similar to counters in mapreduce, if we need to count then we can use accumlator.</p>

<ul>
  <li>shared variable on driver machine</li>
  <li>each executor can only update it but cannot read value</li>
</ul>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">val</span> <span class="nv">v</span> <span class="k">=</span> <span class="nv">sc</span><span class="o">.</span><span class="py">longAccumlator</span><span class="o">(</span><span class="s">"shown name in UI"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">rdd1</span> <span class="k">=</span> <span class="nv">sc</span><span class="o">.</span><span class="py">textFile</span><span class="o">(</span><span class="s">"/path/to/data"</span><span class="o">)</span>

<span class="c1">//it will count the number of lines in file</span>
<span class="nv">rdd1</span><span class="o">.</span><span class="py">foreach</span><span class="o">(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="nv">v</span><span class="o">.</span><span class="py">add</span><span class="o">(</span><span class="mi">1</span><span class="o">))</span>
</code></pre></div></div>

<h3 id="list-to-rdd">list to rdd</h3>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">//lets say we want col1 and count how many times it repeat</span>
<span class="k">val</span> <span class="nv">l</span> <span class="k">=</span> <span class="nc">List</span><span class="o">(</span><span class="s">"A: P1"</span><span class="o">,</span> <span class="s">"B: P2"</span><span class="o">,</span> <span class="s">"A: P3"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">rdd1</span> <span class="k">=</span> <span class="nv">sc</span><span class="o">.</span><span class="py">parallelize</span><span class="o">(</span><span class="n">l</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">pair</span> <span class="k">=</span> <span class="nv">rdd1</span><span class="o">.</span><span class="py">map</span><span class="o">(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="o">{</span>
  <span class="k">val</span> <span class="nv">cols</span> <span class="k">=</span> <span class="nv">x</span><span class="o">.</span><span class="py">split</span><span class="o">(</span><span class="s">":"</span><span class="o">)</span>
  <span class="o">(</span><span class="nf">cols</span><span class="o">(</span><span class="mi">0</span><span class="o">),</span><span class="mi">1</span><span class="o">)</span>
<span class="o">})</span>
<span class="k">val</span> <span class="nv">result</span> <span class="k">=</span> <span class="nv">pair</span><span class="o">.</span><span class="py">reduceByKey</span><span class="o">((</span><span class="n">x</span><span class="o">,</span><span class="n">y</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">x</span><span class="o">+</span><span class="n">y</span><span class="o">)</span>
<span class="nv">result</span><span class="o">.</span><span class="py">collect</span><span class="o">().</span><span class="py">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">)</span>
</code></pre></div></div>

<h3 id="narrow--wide-transformation">narrow &amp; wide transformation</h3>

<p><strong>narrow</strong>: ideally where shuffling is not involved, when rdd partition is dependent on at most one partition of parent rdd</p>

<p>example: map, filter, filtermap, etc</p>

<p><strong>wide</strong>: where shuffling is involved, when rdd partition is dependent on mutiple partitions of parent rdd</p>

<p>example: reduceByKey, groupByKey, etc</p>

<h3 id="job-stages--tasks">job, stages &amp; tasks</h3>

<p><strong>job</strong>:</p>

<ul>
  <li>every action is a job</li>
  <li>sortByKey is exception as its transformation but still shows up as job because some part of it is eager and some part lazy</li>
  <li>when action is called all the transformations in the stage from beginning will be executed. spark optimizes to skip previous stages</li>
</ul>

<p><strong>stage</strong>:</p>

<ul>
  <li>whenever shuffle is required new stage is created, so wide transformation creates new stage</li>
  <li>by default it will be one stage</li>
  <li>when shuffle happens it uses disk so as next stage can pick data</li>
  <li>wide transformations should be used later in the app so as it works on less data</li>
</ul>

<p>example: if 2 wide transormations then total 3 stages in the spark application run</p>

<p><strong>task</strong>:</p>

<ul>
  <li>will be equal to number of partitions read</li>
</ul>

<h3 id="reducebykey-vs-reduce">reduceByKey vs reduce</h3>

<p><strong>reduceByKey</strong></p>

<ul>
  <li>is transormation(wide)</li>
  <li>works on pair rdd (tuple of two: key, value) only</li>
</ul>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">//lets say we want col1 and count how many times it repeat</span>
<span class="k">val</span> <span class="nv">l</span> <span class="k">=</span> <span class="nc">List</span><span class="o">(</span><span class="s">"A: P1"</span><span class="o">,</span> <span class="s">"B: P2"</span><span class="o">,</span> <span class="s">"A: P3"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">rdd1</span> <span class="k">=</span> <span class="nv">sc</span><span class="o">.</span><span class="py">parallelize</span><span class="o">(</span><span class="n">l</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">pair</span> <span class="k">=</span> <span class="nv">rdd1</span><span class="o">.</span><span class="py">map</span><span class="o">(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="o">{</span>
  <span class="k">val</span> <span class="nv">cols</span> <span class="k">=</span> <span class="nv">x</span><span class="o">.</span><span class="py">split</span><span class="o">(</span><span class="s">":"</span><span class="o">)</span>
  <span class="o">(</span><span class="nf">cols</span><span class="o">(</span><span class="mi">0</span><span class="o">),</span><span class="mi">1</span><span class="o">)</span>
<span class="o">})</span>

<span class="c1">//transformation: here we get only rdd</span>
<span class="k">val</span> <span class="nv">result</span> <span class="k">=</span> <span class="nv">pair</span><span class="o">.</span><span class="py">reduceByKey</span><span class="o">((</span><span class="n">x</span><span class="o">,</span><span class="n">y</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">x</span><span class="o">+</span><span class="n">y</span><span class="o">)</span>

<span class="c1">//action: here we get result on local not rdd</span>
<span class="nv">result</span><span class="o">.</span><span class="py">collect</span><span class="o">().</span><span class="py">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">)</span>
</code></pre></div></div>

<p><strong>reduce</strong>:</p>

<ul>
  <li>is an action</li>
  <li>gives single output</li>
</ul>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">val</span> <span class="nv">a</span> <span class="k">=</span> <span class="mi">1</span> <span class="n">to</span> <span class="mi">100</span>
<span class="k">val</span> <span class="nv">rdd1</span> <span class="k">=</span> <span class="nv">sc</span><span class="o">.</span><span class="py">parallelize</span><span class="o">(</span><span class="n">a</span><span class="o">)</span>

<span class="c1">//action: it will return result</span>
<span class="nv">rdd1</span><span class="o">.</span><span class="py">reduce</span><span class="o">((</span><span class="n">x</span><span class="o">,</span><span class="n">y</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">x</span><span class="o">+</span><span class="n">y</span><span class="o">)</span>
</code></pre></div></div>

<h3 id="reducebykey-vs-groupbykey">reduceByKey vs groupByKey</h3>

<p>both are transformation(wide)</p>

<p><strong>reduceByKey</strong>:</p>

<ul>
  <li>works on mapper end to do local aggregations</li>
  <li>similar to combiner in hadoop</li>
  <li>less shuffling as local aggregations done</li>
</ul>

<p><strong>groupByKey</strong>:</p>

<ul>
  <li>all the data is sent to reducer to do aggregations</li>
  <li>more shuffling</li>
  <li>should be avoided as no local aggregations</li>
  <li>resources will be wasted as most of mappers will be ideal in case of reducer work</li>
</ul>

<p><strong>Note</strong>: by default, hdfs blocks size is 128MB and local block size is 32MB</p>

<h3 id="pair-rdd-vs-map-datatype-in-scala">pair rdd vs map datatype in scala</h3>

<p>pait rdd holds tuple of two elements(key, value).</p>

<p>map datatype in scala also hold key value pair but cannot have duplicates while pair rdd can have duplicates</p>

<h3 id="save-to-file">save to file</h3>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">//its an action</span>
<span class="nv">rdd</span><span class="o">.</span><span class="py">saveAsTextFile</span><span class="o">(</span><span class="s">"/path/to/file"</span><span class="o">)</span>
</code></pre></div></div>

<h3 id="parallelism-and-partitions">parallelism and partitions</h3>

<p>default parallism is equal to number of cores and default partitions is equal to number of parallelism</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">//to check parallelism</span>
<span class="nv">sc</span><span class="o">.</span><span class="py">defaultParallelism</span>

<span class="k">val</span> <span class="nv">l</span> <span class="k">=</span> <span class="nc">List</span><span class="o">(</span><span class="s">"A: P1"</span><span class="o">,</span> <span class="s">"B: P2"</span><span class="o">,</span> <span class="s">"A: P3"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">rdd1</span> <span class="k">=</span> <span class="nv">sc</span><span class="o">.</span><span class="py">parallelize</span><span class="o">(</span><span class="n">l</span><span class="o">)</span>

<span class="c1">//to check number of partitions</span>
<span class="nv">rdd1</span><span class="o">.</span><span class="py">getNumPartitions</span>

<span class="c1">//if we load from local file in rdd than it will use 32MB block to partition</span>
<span class="c1">//if hdfs than 128MB</span>
<span class="c1">//but if file is small like 2MB than default min partition is used which is 2</span>

<span class="nv">sc</span><span class="o">.</span><span class="py">defaultMinPartitions</span>
</code></pre></div></div>

<h3 id="repartition-vs-coalesce">repartition vs coalesce</h3>

<p><strong>repartition</strong>:</p>

<ul>
  <li>to increase parallelism we may need to repartition to more partitions</li>
  <li>if we have less data in each partition after a certain point then we may need to consider repartition to use less partitions with good data</li>
  <li>it is wide transformation</li>
  <li>does full shuffle of data to get exact equal size</li>
</ul>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">//lets say rdd1 exist with default 2 partitions</span>
<span class="nv">rdd1</span><span class="o">.</span><span class="py">getNumPartitions</span>

<span class="c1">//this will repartition in 10 partitions</span>
<span class="k">val</span> <span class="nv">rdd2</span> <span class="k">=</span> <span class="nv">rdd1</span><span class="o">.</span><span class="py">repartition</span><span class="o">(</span><span class="mi">10</span><span class="o">)</span>

<span class="nv">rdd2</span><span class="o">.</span><span class="py">getNumPartitions</span>


<span class="c1">//decrease number of partitions to 1</span>
<span class="k">val</span> <span class="nv">rdd3</span> <span class="k">=</span> <span class="nv">rdd1</span><span class="o">.</span><span class="py">repartition</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
</code></pre></div></div>

<p><strong>coalesce</strong>:</p>

<ul>
  <li>same as repartition but can only decrease number of partitions</li>
  <li>increasing does not give error but also does not change the number of partitions</li>
  <li>it is transformation</li>
  <li>minimize shuffling by combining local partitions to avoid full shuffle</li>
</ul>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">//lets say rdd1 exist with default 2 partitions</span>
<span class="nv">rdd1</span><span class="o">.</span><span class="py">getNumPartitions</span>

<span class="c1">//decrease number of partitions to 1</span>
<span class="k">val</span> <span class="nv">rdd2</span> <span class="k">=</span> <span class="nv">rdd1</span><span class="o">.</span><span class="py">coalesce</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>

<span class="nv">rdd2</span><span class="o">.</span><span class="py">getNumPartitions</span>
</code></pre></div></div>

<p><strong>Note</strong>: to decrease repartition and coalesce both can be used but coalesce if preferred as it minimizes shuffling by trying to combine partitions within the node first</p>

<h3 id="cache--persist">cache &amp; persist</h3>

<p>if we have a lot of transormations and we call actions mutiple times then all the transformations will run again or atleast the last stage will be executed in full. If we use cache then transormations (if any) or action after cache will only be executed.</p>

<p>cache and persist, both have same purpose to not do all the transformations again.</p>

<p>cache will always be in memory, persist have option of various storage levels. persist in memory <code class="language-plaintext highlighter-rouge">persist()</code> is same as cache.</p>

<h4 id="persist-storage-levels">persist storage levels</h4>

<p><strong>MEMORY_ONLY</strong>: cached in memory non serialized format</p>

<p><strong>DISK_ONLY</strong>: in disk serialized format</p>

<p><strong>MEMORY_AND_DISK</strong>: in memory, if not enough memory then evicts blocks and are stored in disk. recommemded to avoid expensive recalculation and memory is limited</p>

<p><strong>OFF_HEAP</strong>: blocks cached off heap (outside jvm), in jvm it uses garbage collection to free space, its time taking process. So, we could uses memory outside executor. These are called unsafe operations as it will use raw memory outside jvm. It stores in serialized format</p>

<p><strong>MEMORY_ONLY_SER</strong>: same as memory only but serialized</p>

<p><strong>MEMORY_AND_DISK_SER</strong>: same as memory and disk but serialized</p>

<p><strong>MEMORY_ONLY_2</strong>: same as memory only but with 2 replicas on different worker nodes, t speed up recovery</p>

<p><strong>Note</strong>:</p>
<ul>
  <li>In memory only if we dont have enough memory then it will not fail rather will skip caching it.</li>
  <li>serialized saves space as it stores in binary but will require more processing</li>
</ul>

<h3 id="run-jar">run jar</h3>

<p>jar is Java ARchive, a package file format typically used to aggregate many java class files and associated metadata and resources (text, images, etc.) into one file for distribution. <a href="https://en.wikipedia.org/wiki/JAR_(file_format)">jar-wiki</a></p>

<p>to run from terminal we can submit like below</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>spark-submit <span class="nt">--class</span> ClassName path/to/jar
</code></pre></div></div>

<h3 id="map-vs-map-partition">map vs map partition</h3>

<p>map is a transformation which works on each row. for example if in rdd there are 4 partitins with 1000 rows in each partition then map will be called 4000 times</p>

<p>map partition works on each partition so in example above it will be run 4 times. This can help if we need to make connection to database</p>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#hadoop" class="page__taxonomy-item" rel="tag">hadoop</a><span class="sep">, </span>
    
      <a href="/tags/#hdfs" class="page__taxonomy-item" rel="tag">hdfs</a><span class="sep">, </span>
    
      <a href="/tags/#pyspark" class="page__taxonomy-item" rel="tag">pyspark</a><span class="sep">, </span>
    
      <a href="/tags/#scala" class="page__taxonomy-item" rel="tag">scala</a><span class="sep">, </span>
    
      <a href="/tags/#spark" class="page__taxonomy-item" rel="tag">spark</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#blog" class="page__taxonomy-item" rel="tag">blog</a>
    
    </span>
  </p>


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2022-02-04T17:00:00+01:00">February 4, 2022</time></p>


      </footer>

      

      
  <nav class="pagination">
    
      <a href="/blog/scala-part-II/" class="pagination--pager" title="scala part II
">Previous</a>
    
    
      <a href="/blog/python/" class="pagination--pager" title="python basics
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You May Also Enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/blog/spark-optimizations/" rel="permalink">spark optimizations
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          2 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">optimizations can be at application code level or at cluster level, here we are looking more at cluster level optimizations

</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/blog/spark-part-II/" rel="permalink">spark part-II
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          9 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">spark core works on rdds (spark 1 style) but we have high level constructs to query/process data easily, its dataframe/datasets

</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/blog/hive-basics/" rel="permalink">hive basics
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          7 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">it is open source datawarehouse to process structured data on top of hadoop

</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/blog/yarn/" rel="permalink">yarn
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          3 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Yet Another Resource Negotiator
Lets first go through how things are in hadoop initial version and what the limitations are which is solved by YARN.

</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://www.linkedin.com/in/balpreet-singh-654705114" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> Linkedin</a></li>
        
      
        
          <li><a href="https://github.com/khuranabal?tab=repositories" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2022 Balpreet Singh. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




    <script>
  'use strict';

  (function() {
    var commentContainer = document.querySelector('#utterances-comments');

    if (!commentContainer) {
      return;
    }

    var script = document.createElement('script');
    script.setAttribute('src', 'https://utteranc.es/client.js');
    script.setAttribute('repo', 'khuranabal/khuranabal.github.io');
    script.setAttribute('issue-term', 'pathname');
    script.setAttribute('theme', 'github-light');
    script.setAttribute('crossorigin', 'anonymous');

    commentContainer.appendChild(script);
  })();
</script>

  





  </body>
</html>
