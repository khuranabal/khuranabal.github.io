<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-01-15T17:36:22+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Balpreet Singh</title><subtitle>Data Engineer blogs Azure data services like data lake, blob, databricks, database, sql server, terraform, docker, kubernetes, SQL, Python, HCL Balpreet Singh</subtitle><author><name>{&quot;avatar&quot;=&gt;&quot;/assets/images/bio-photo.jpg&quot;, &quot;bio&quot;=&gt;&quot;Senior Data Engineer&quot;, &quot;links&quot;=&gt;[{&quot;label&quot;=&gt;&quot;Website&quot;, &quot;icon&quot;=&gt;&quot;fas fa-fw fa-link&quot;, &quot;url&quot;=&gt;&quot;https://khuranabal.github.io/&quot;}, {&quot;label&quot;=&gt;&quot;Linkedin&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-linkedin&quot;, &quot;url&quot;=&gt;&quot;https://www.linkedin.com/in/balpreet-singh-654705114&quot;}, {&quot;label&quot;=&gt;&quot;GitHub&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-github&quot;, &quot;url&quot;=&gt;&quot;https://github.com/khuranabal?tab=repositories&quot;}]}</name></author><entry><title type="html">file formats for big data</title><link href="http://localhost:4000/blog/file-formats/" rel="alternate" type="text/html" title="file formats for big data" /><published>2022-01-14T16:33:00+01:00</published><updated>2022-01-14T16:33:00+01:00</updated><id>http://localhost:4000/blog/file-formats</id><content type="html" xml:base="http://localhost:4000/blog/file-formats/">&lt;p&gt;There are certain parameters to consider when chossing a file format.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;storage space consumption&lt;/li&gt;
  &lt;li&gt;processing time&lt;/li&gt;
  &lt;li&gt;io consumption&lt;/li&gt;
  &lt;li&gt;read/write speed&lt;/li&gt;
  &lt;li&gt;if data can be split in files&lt;/li&gt;
  &lt;li&gt;schema evolution&lt;/li&gt;
  &lt;li&gt;advanced compression&lt;/li&gt;
  &lt;li&gt;compatible with framework like hive/spark etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;ways-in-which-data-can-be-stored&quot;&gt;ways in which data can be stored&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;row based&lt;/strong&gt;: write is simple but read will have to read full row even if subset of column is read.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;column based&lt;/strong&gt;: all columns values are stored together. Write will be slower comparatively. But read will be efficient. In this we can get good compression as well.&lt;/p&gt;

&lt;h3 id=&quot;file-formats&quot;&gt;file formats&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;csv&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;all data stored as text so takes a lot of storage for example if column has integer value then that consumes more storage because its stored as text.&lt;/li&gt;
  &lt;li&gt;processing will be slow as conversion need to be done.&lt;/li&gt;
  &lt;li&gt;io will be slow as data storage is more so will do more io.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;xml&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;semi structure&lt;/li&gt;
  &lt;li&gt;all negative of csv applies here as well.&lt;/li&gt;
  &lt;li&gt;these files can not split.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;json&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;semi structure&lt;/li&gt;
  &lt;li&gt;all negative of csv applies here as well.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;avro&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;row based storage&lt;/li&gt;
  &lt;li&gt;faster write as its row based&lt;/li&gt;
  &lt;li&gt;slow read for subset of columns&lt;/li&gt;
  &lt;li&gt;schema of file is stored in json&lt;/li&gt;
  &lt;li&gt;data is self describing because schema is embeded as part of data&lt;/li&gt;
  &lt;li&gt;actual data is in binary format&lt;/li&gt;
  &lt;li&gt;general file format, programming language agnostic can used in many languages&lt;/li&gt;
  &lt;li&gt;matuare in schema evolution&lt;/li&gt;
  &lt;li&gt;serialization format&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;orc&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;optimized row columnar&lt;/li&gt;
  &lt;li&gt;write are not effecient&lt;/li&gt;
  &lt;li&gt;effecient reads&lt;/li&gt;
  &lt;li&gt;highly effecient in terms of storage&lt;/li&gt;
  &lt;li&gt;compression (dictionary encoding, bit packing, delta encoding, run length encoding along with generalzed compression like snappy/gzip)&lt;/li&gt;
  &lt;li&gt;predicate pushdown&lt;/li&gt;
  &lt;li&gt;best fit for hive, supports all datatypes including complex used in hive&lt;/li&gt;
  &lt;li&gt;initially specially designed for hive&lt;/li&gt;
  &lt;li&gt;supports schema evolution, not matuare as avro&lt;/li&gt;
  &lt;li&gt;self describing, as stores metadata(using protocol buffers) in the end of file itself&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;parquet&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;column based storage&lt;/li&gt;
  &lt;li&gt;writes are not effecient&lt;/li&gt;
  &lt;li&gt;effecient reads&lt;/li&gt;
  &lt;li&gt;shares many design patterns as of orc, but more general purpose&lt;/li&gt;
  &lt;li&gt;very good in handling nested data&lt;/li&gt;
  &lt;li&gt;compression is effecient&lt;/li&gt;
  &lt;li&gt;self describing, as stores metadata in the end of file itself&lt;/li&gt;
  &lt;li&gt;supports schema evolution adding/removing columns in the end&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;orc-storage-internals&quot;&gt;orc storage internals&lt;/h3&gt;

&lt;p&gt;Data is stored as shown in the below image. Mainly it has below sections.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Header&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;It contains text &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ORC&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Body&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In it data is divided in multiple stripes (default size is 250MB) and each stripe has-&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Index data: max, min, count of each column in every row group in the stripe&lt;/li&gt;
  &lt;li&gt;Row data: data is broken in row groups each row group has 10000 rows by default&lt;/li&gt;
  &lt;li&gt;Stripe footer: stores encoding used&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Footer&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;File footer: contains metadata at file and stripe level like max, min, count.&lt;/li&gt;
  &lt;li&gt;Postscript: stores which compression is used like snappy/gzip, postscipt is never compressed&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/fileformats/orc.png&quot; alt=&quot;orc internals&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Flow is like header is read to identify orc file and then postscript is read to get compression used and then file footer then stripes and row data.&lt;/p&gt;

&lt;h3 id=&quot;parquet-storage-internals&quot;&gt;parquet storage internals&lt;/h3&gt;

&lt;p&gt;Data is stored as shown in the below image. Mainly it has below sections.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Header&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;It contains text &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PAR1&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Row group&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In it data is divided in column chunks which is further divided in pages.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Footer&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;File metadata: encoding, schema, etc.&lt;/li&gt;
  &lt;li&gt;Lenght of file metadata&lt;/li&gt;
  &lt;li&gt;Magic number &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PAR1&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/fileformats/parquet.png&quot; alt=&quot;parquet internals&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;compression&quot;&gt;compression&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;dictionary encoding&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Suppose we have sales data where product name column, customer name column exist then that will have same values for a lot of rows. For this dictonary encoding helps by having dictionary where distinct vaules are stored and then referenced.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;bit packing&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Suppose we have int column in the dateset, for that column if the vaule is less than bit packing can help by representing same number with less bits.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;delta encoding&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Suppose we have timestamp column in dataset then first timestamp is stored and then for next column vaule it can store difference only. Example vaule is 123456 and next vaule is 123457 then base vaule stored can be 123456 and next value 1.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;run length encoding&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Suppose we have column in dataset which has value dddddfffgg then the vaule stored is d5f3g2&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;serialization is converting data into a form which can be easily transferred over the network and stored in file system.&lt;/li&gt;
  &lt;li&gt;there is no other file format better than avro for schema evolution.&lt;/li&gt;
  &lt;li&gt;avo can be best fit for landing in data lake as raw data.&lt;/li&gt;
  &lt;li&gt;in avro, orc, parquet any compression can be used, compression code is stored in metadata, so reader can get to know compression code from metadata.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;sources&quot;&gt;Sources&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;https://orc.apache.org/specification/ORCv2/&lt;/li&gt;
  &lt;li&gt;https://parquet.apache.org/documentation/latest/&lt;/li&gt;
&lt;/ul&gt;</content><author><name>{&quot;avatar&quot;=&gt;&quot;/assets/images/bio-photo.jpg&quot;, &quot;bio&quot;=&gt;&quot;Senior Data Engineer&quot;, &quot;links&quot;=&gt;[{&quot;label&quot;=&gt;&quot;Website&quot;, &quot;icon&quot;=&gt;&quot;fas fa-fw fa-link&quot;, &quot;url&quot;=&gt;&quot;https://khuranabal.github.io/&quot;}, {&quot;label&quot;=&gt;&quot;Linkedin&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-linkedin&quot;, &quot;url&quot;=&gt;&quot;https://www.linkedin.com/in/balpreet-singh-654705114&quot;}, {&quot;label&quot;=&gt;&quot;GitHub&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-github&quot;, &quot;url&quot;=&gt;&quot;https://github.com/khuranabal?tab=repositories&quot;}]}</name></author><category term="blog" /><category term="csv" /><category term="xml" /><category term="json" /><category term="avro" /><category term="orc" /><category term="parquet" /><category term="delta" /><summary type="html">There are certain parameters to consider when chossing a file format.</summary></entry><entry><title type="html">sqoop working</title><link href="http://localhost:4000/blog/sqoop/" rel="alternate" type="text/html" title="sqoop working" /><published>2022-01-12T17:16:00+01:00</published><updated>2022-01-12T17:16:00+01:00</updated><id>http://localhost:4000/blog/sqoop</id><content type="html" xml:base="http://localhost:4000/blog/sqoop/">&lt;p&gt;It is used to transfer data between rdbms to hdfs and vice versa.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;sqoop import: transfer from rdbms to hdfs&lt;/li&gt;
  &lt;li&gt;sqoop export: transfer from hdfs to rdbms&lt;/li&gt;
  &lt;li&gt;sqoop eval: to run queries on databases&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;setup-cloudera-quickstart-machine-on-local&quot;&gt;Setup cloudera quickstart machine on local&lt;/h2&gt;
&lt;p&gt;We have multiple ways to setup cloudera machine either as VM or docker image, in this case we will use docker and local machine is ubuntu. So, prerequisite is to:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;setup docker, follow &lt;a href=&quot;https://docs.docker.com/engine/install/ubuntu/&quot;&gt;here&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;run below to pull cloudera quickstart image and then run image which will setup cloudera machine to interact via terminal&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker pull cloudera/quickstart:latest
docker run &lt;span class=&quot;nt&quot;&gt;--hostname&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;quickstart.cloudera &lt;span class=&quot;nt&quot;&gt;--privileged&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-i&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 8888:8888 &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 80:80 &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 8080:8080 cloudera/quickstart /usr/bin/docker-quickstart
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;commands&quot;&gt;Commands&lt;/h2&gt;

&lt;h3 id=&quot;list-databases&quot;&gt;list databases&lt;/h3&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sqoop-list-databases &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--connect&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;jdbc:mysql://quickstart.cloudera:3306&quot;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--username&lt;/span&gt; root &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--password&lt;/span&gt; cloudera
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;list-tables&quot;&gt;list tables&lt;/h3&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sqoop-list-tables &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--connect&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;jdbc:mysql://quickstart.cloudera:3306/retail_db&quot;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--username&lt;/span&gt; root &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--password&lt;/span&gt; cloudera
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;query-database&quot;&gt;query database&lt;/h3&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sqoop-eval &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--connect&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;jdbc:mysql://quickstart.cloudera:3306&quot;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--username&lt;/span&gt; root &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--password&lt;/span&gt; cloudera &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--query&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;select * from retail_db.customers limit 5&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;import-one-table&quot;&gt;import one table&lt;/h3&gt;
&lt;p&gt;It will run mapreduce job where only mappers will work. Default four mappers will run but can be changed. It divide work based on primary key. If no primary key then command will failm in that case we have two options:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;change number of mapper to one: use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-m 1&lt;/code&gt; parameter&lt;/li&gt;
  &lt;li&gt;use split by column.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Example: Here &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;orders&lt;/code&gt; table has primary key and we testing import on this table.&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sqoop import &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--connect&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;jdbc:mysql://quickstart.cloudera:3306/retail_db&quot;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--username&lt;/span&gt; root &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--password&lt;/span&gt; cloudera &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--table&lt;/span&gt; orders &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--target-dir&lt;/span&gt; /result
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Run below command after above import command to check files produced by import command, default number of mappers are 4 so thats why it will have four part-m files.&lt;/p&gt;
&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;hadoop fs &lt;span class=&quot;nt&quot;&gt;-ls&lt;/span&gt; /result
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;import-all-tables&quot;&gt;import all tables&lt;/h3&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sqoop-import-all-tables &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--connect&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;jdbc:mysql://quickstart.cloudera:3306/retail_db&quot;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--username&lt;/span&gt; root &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--password&lt;/span&gt; cloudera &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--table&lt;/span&gt; orders &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--warehouse-dir&lt;/span&gt; /result
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;target-dir: files will directly be stored in the directory mentioned in command.&lt;/li&gt;
  &lt;li&gt;warehouse-dir: files will be stored in the subdirectory (same as tablename) of the directory mentioned in command.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note: sqoop is retired as of June 2021&lt;/p&gt;

&lt;p&gt;Sources:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Cloudera quickstart docker image for testing, &lt;a href=&quot;https://hub.docker.com/r/cloudera/quickstart/&quot;&gt;source&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>{&quot;avatar&quot;=&gt;&quot;/assets/images/bio-photo.jpg&quot;, &quot;bio&quot;=&gt;&quot;Senior Data Engineer&quot;, &quot;links&quot;=&gt;[{&quot;label&quot;=&gt;&quot;Website&quot;, &quot;icon&quot;=&gt;&quot;fas fa-fw fa-link&quot;, &quot;url&quot;=&gt;&quot;https://khuranabal.github.io/&quot;}, {&quot;label&quot;=&gt;&quot;Linkedin&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-linkedin&quot;, &quot;url&quot;=&gt;&quot;https://www.linkedin.com/in/balpreet-singh-654705114&quot;}, {&quot;label&quot;=&gt;&quot;GitHub&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-github&quot;, &quot;url&quot;=&gt;&quot;https://github.com/khuranabal?tab=repositories&quot;}]}</name></author><category term="blog" /><category term="sqoop" /><category term="hdfs" /><summary type="html">It is used to transfer data between rdbms to hdfs and vice versa.</summary></entry><entry><title type="html">MapReduce working</title><link href="http://localhost:4000/blog/map-reduce/" rel="alternate" type="text/html" title="MapReduce working" /><published>2022-01-11T15:13:00+01:00</published><updated>2022-01-11T15:13:00+01:00</updated><id>http://localhost:4000/blog/map-reduce</id><content type="html" xml:base="http://localhost:4000/blog/map-reduce/">&lt;p&gt;MapReduce is programming model for processing big datsets. It consists of two stages:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;map&lt;/li&gt;
  &lt;li&gt;reduce&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Both works on key value pairs.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;key&lt;/td&gt;
      &lt;td&gt;value&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;id&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;name&lt;/td&gt;
      &lt;td&gt;abc&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/mapreduce/mapreduce-input-output.png&quot; alt=&quot;mapreduce input output&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;understanding-with-example&quot;&gt;Understanding with example&lt;/h3&gt;

&lt;p&gt;Lets say we have a big file (size 100GB) and we need to find frequency of words.
By default block size is 128MB so data will be divided as per block size and distributed among nodes. Considering with default one reducer.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/mapreduce/sample-input-output.png&quot; alt=&quot;sample data&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Clearly the input is not in key value format, so here record reader is used to read input file and convert each line in key vaule pair. Here each line will be assigned a key by record reader and the vaule will be the actual line content.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Mapper logic&lt;/strong&gt;: Just take the vaules, and split on space to find words and assign word as key and 1 as vaule.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Shuffle &amp;amp; Sort&lt;/strong&gt;: Transfer all the key vaule pairs on to single machine (reduer). Sorting will happen on keys, for example output will be in format: (key, {vaule, vaule, …})&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Reducer logic&lt;/strong&gt;: Iterate over list of vaules in a key and sum it up.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/mapreduce/processing.png&quot; alt=&quot;processing&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Number of mappers will be equal to number of data blocks.&lt;/li&gt;
  &lt;li&gt;Number of reducer is set by developer default is 1 but we can set to 0 or any higher number.&lt;/li&gt;
  &lt;li&gt;System take care of shuffle and sort operation.&lt;/li&gt;
  &lt;li&gt;Record reader works on mapper machine.&lt;/li&gt;
  &lt;li&gt;If number of reducers are 0 then shuffle and sort will not execute.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;optimization&quot;&gt;Optimization&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;More work on mappers&lt;/strong&gt;: It will optimize the processing as all mappers are executed parellely. More on this below in combiner section.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Increase reducer&lt;/strong&gt;: When we cannot put more work in the mapper then we can think of increasing reducer. Example: if all mappers take 1 min and reducer takes 4 min then we can consider increasing number of reducer in order to optimize the run time.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;No reducer&lt;/strong&gt;: Where no aggregation required, example: filtering.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;No shuffle and sort will be executed if reducer is not used.&lt;/li&gt;
  &lt;li&gt;If we have more than one reducer then output of each mapper is divided in n partitions if n reducers are in cluster, partition 1 from each mapper goes to reducer 1 and so on. Hash function is used to route which key value goes to which partition. Hash function is consistent so as all same keys are in the same reducer.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;combiner&quot;&gt;Combiner&lt;/h4&gt;

&lt;p&gt;In some cases, when work is moved more on the mapper that means we can have some aggreation work on mapper. Like if we need max value of the key then that can be performed on mapper as well. And the aggregation which is done on mapper is done by combiner process, it is executed on mapper machines itself.&lt;/p&gt;

&lt;p&gt;But avoid using in case where data can change when executing on one machine compared to multiple machine like avg.&lt;/p&gt;

&lt;p&gt;Note: Combiner logic is same as reducer logic in this case for example: max, min, sum.&lt;/p&gt;

&lt;h3 id=&quot;flow-of-data&quot;&gt;Flow of data&lt;/h3&gt;

&lt;p&gt;record reader -&amp;gt; mapper -&amp;gt; combiner -&amp;gt; partitioner -&amp;gt; shuffle &amp;amp; sort -&amp;gt; reducer&lt;/p&gt;</content><author><name>{&quot;avatar&quot;=&gt;&quot;/assets/images/bio-photo.jpg&quot;, &quot;bio&quot;=&gt;&quot;Senior Data Engineer&quot;, &quot;links&quot;=&gt;[{&quot;label&quot;=&gt;&quot;Website&quot;, &quot;icon&quot;=&gt;&quot;fas fa-fw fa-link&quot;, &quot;url&quot;=&gt;&quot;https://khuranabal.github.io/&quot;}, {&quot;label&quot;=&gt;&quot;Linkedin&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-linkedin&quot;, &quot;url&quot;=&gt;&quot;https://www.linkedin.com/in/balpreet-singh-654705114&quot;}, {&quot;label&quot;=&gt;&quot;GitHub&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-github&quot;, &quot;url&quot;=&gt;&quot;https://github.com/khuranabal?tab=repositories&quot;}]}</name></author><category term="blog" /><category term="mapreduce" /><category term="hadoop" /><summary type="html">MapReduce is programming model for processing big datsets. It consists of two stages: map reduce</summary></entry><entry><title type="html">hdfs architecture</title><link href="http://localhost:4000/blog/understand-hdfs/" rel="alternate" type="text/html" title="hdfs architecture" /><published>2022-01-08T14:45:00+01:00</published><updated>2022-01-08T14:45:00+01:00</updated><id>http://localhost:4000/blog/understand-hdfs</id><content type="html" xml:base="http://localhost:4000/blog/understand-hdfs/">&lt;p&gt;hdfs is hadoop distributed file system. Highly fault tolerant and is designed to deploy on low cost machines.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hdfs/hdfs-architecture.png&quot; alt=&quot;hdfs architecture&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let’s assume we have cluster(2 data node, 1 name node) and file of 250MB.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Default block size: 128MB&lt;/li&gt;
  &lt;li&gt;Default replication factor: 3&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So, there will be 2 blocks:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Block-1: 128MB&lt;/li&gt;
  &lt;li&gt;Block-2: 122MB&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When data will be loaded then both the blocks will be loaded on different data node, behind the scene as per default setting each block will be replicated 3 times, in this case 1 node will have 2 copies of block and other node will have 1 copy of block.&lt;/p&gt;

&lt;p&gt;Note:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;last block might have less than 128MB depending on the size of file, but space is not wasted as remaining space will free up and is used by other blocks.&lt;/li&gt;
  &lt;li&gt;one data block cannot be shared by two files, so last block of one file is not used by other file bock&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;metadata-stored-in-name-node&quot;&gt;Metadata stored in name node&lt;/h3&gt;
&lt;p&gt;At high level below details are stored in metadata.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Filename&lt;/li&gt;
  &lt;li&gt;Block&lt;/li&gt;
  &lt;li&gt;Data node: All the data nodes which have the block&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Example:&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;File name&lt;/td&gt;
      &lt;td&gt;Block Number&lt;/td&gt;
      &lt;td&gt;Data node 1&lt;/td&gt;
      &lt;td&gt;Data Node 2&lt;/td&gt;
      &lt;td&gt;Data Node 3&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;data-node-failure&quot;&gt;Data node failure&lt;/h3&gt;
&lt;p&gt;In case of any failue in data node other data node will be responsible for serving requests and name node will maintain the copies as per replication factor.&lt;/p&gt;

&lt;h4 id=&quot;1-dead-data-node&quot;&gt;1. Dead data node&lt;/h4&gt;
&lt;p&gt;Each data node sends heart beat every 3 seconds to name node and if 10 consecutive times hear beat is not received from a data node then data node is treated as dead or running very slow.&lt;/p&gt;

&lt;h4 id=&quot;2-corrupt-data-node&quot;&gt;2. Corrupt data node&lt;/h4&gt;
&lt;p&gt;Each data node sends block report using which name node can identify if block is corrupted then will replicate the block from known good replica and mark the corrupted one for delete.&lt;/p&gt;

&lt;h3 id=&quot;name-node-failure&quot;&gt;Name node failure&lt;/h3&gt;
&lt;p&gt;In case of failure of name node there will be no access to metadata. So in hadoop v1 it was single point of failure. In hadoop v2 it has secondary name node so it’s not single point of failure.
Important metadata files:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;fsimage: snapshot of file system at a particular time&lt;/li&gt;
  &lt;li&gt;edit logs: transaction logs that changes in hdfs file system&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Secondary name node is responsible for merging fsimage with edit logs and generating new fsimage, once merge is done it restes fsimage and edit logs to empty. This process is done every 30 sec.&lt;/p&gt;

&lt;p&gt;So, when primary name node fails, secondary can become active and then we have to introduce secondary name node.&lt;/p&gt;

&lt;h3 id=&quot;block-size&quot;&gt;Block size&lt;/h3&gt;
&lt;p&gt;In hadoop v2 default block size is 128MB, if it is increased then we might have less blocks to process then nodes so cluster might be under utilized, if block size is decreased then we might have a lot of blocks which will increse burden on name node to keep track of metadata and handling it.&lt;/p&gt;

&lt;h3 id=&quot;hadoop-ha&quot;&gt;Hadoop HA&lt;/h3&gt;
&lt;p&gt;High availibility is achieved by two name nodes primary and hot standby. If primary goes down then standby takes overs. It can be achieved using quorum journal manager.&lt;/p&gt;

&lt;p&gt;Three journal nodes are used and metadata is copied over from active name node and here secondary is called as standby name node and still does the same checkpointing work to combine fsimage snaphot with edit log reading from journal nodes. Standby keeps lock on jk(zookeeper) and always check if its locked or not, if not then takes lock and becomes active.&lt;/p&gt;

&lt;p&gt;Note: Three journal nodes are used to tolerate single machine failure. The system can tolerate at most (N-1) / 2 failures when running with N JournalNodes.&lt;/p&gt;</content><author><name>{&quot;avatar&quot;=&gt;&quot;/assets/images/bio-photo.jpg&quot;, &quot;bio&quot;=&gt;&quot;Senior Data Engineer&quot;, &quot;links&quot;=&gt;[{&quot;label&quot;=&gt;&quot;Website&quot;, &quot;icon&quot;=&gt;&quot;fas fa-fw fa-link&quot;, &quot;url&quot;=&gt;&quot;https://khuranabal.github.io/&quot;}, {&quot;label&quot;=&gt;&quot;Linkedin&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-linkedin&quot;, &quot;url&quot;=&gt;&quot;https://www.linkedin.com/in/balpreet-singh-654705114&quot;}, {&quot;label&quot;=&gt;&quot;GitHub&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-github&quot;, &quot;url&quot;=&gt;&quot;https://github.com/khuranabal?tab=repositories&quot;}]}</name></author><category term="blog" /><category term="hdfs" /><category term="hadoop" /><summary type="html">hdfs is hadoop distributed file system. Highly fault tolerant and is designed to deploy on low cost machines.</summary></entry><entry><title type="html">Build and deploy static website</title><link href="http://localhost:4000/blog/setup-github-pages-with-jekyll/" rel="alternate" type="text/html" title="Build and deploy static website" /><published>2022-01-04T16:00:00+01:00</published><updated>2022-01-04T16:00:00+01:00</updated><id>http://localhost:4000/blog/setup-github-pages-with-jekyll</id><content type="html" xml:base="http://localhost:4000/blog/setup-github-pages-with-jekyll/">&lt;p&gt;To host website we have different ways and for this blog we are focussing on a use case where we need to have a website for blogging, and we are using:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;jekyll: static site generator, theme used is minimal mistakes&lt;/li&gt;
  &lt;li&gt;github pages: static website hosting service&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;installation&quot;&gt;Installation&lt;/h3&gt;
&lt;p&gt;Following are required to be installed in order to build and test the static website locally.
Installation instructions &lt;a href=&quot;https://jekyllrb.com/docs/installation/&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;ruby&lt;/li&gt;
  &lt;li&gt;rubygems&lt;/li&gt;
  &lt;li&gt;gcc&lt;/li&gt;
  &lt;li&gt;make&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;prerequisite&quot;&gt;Prerequisite&lt;/h3&gt;
&lt;p&gt;Use template repo to get started, we are using minimal-mistakes theme of jeykell, feel free to use any other theme as per need. Download &lt;a href=&quot;https://github.com/mmistakes/mm-github-pages-starter&quot;&gt;repo&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Test if its working fine on local by executing below command and then browsing on localhost:4000&lt;/p&gt;
&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bundle &lt;span class=&quot;nb&quot;&gt;install
&lt;/span&gt;bundle &lt;span class=&quot;nb&quot;&gt;exec &lt;/span&gt;jekyll serve
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note: config/post can be modified/added as required, follow &lt;a href=&quot;https://mmistakes.github.io/minimal-mistakes/docs/configuration/&quot;&gt;docs&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;configure-pages-and-deploy-to-github&quot;&gt;Configure pages and deploy to github&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;create new repo and configure github settings to enable pages, follow &lt;a href=&quot;https://docs.github.com/en/pages/quickstart&quot;&gt;here&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;push code to the new created repo&lt;/li&gt;
  &lt;li&gt;access githubpages url, if the repo created is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;username.github.io&lt;/code&gt; then url will be &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;https:\\username.github.io&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;enabling-feature-to-add-comments-in-the-blogs&quot;&gt;Enabling feature to add comments in the blogs&lt;/h3&gt;

&lt;p&gt;In order to enable comments feature we have different options and here we have used github issues where all the comments in the blog will be stored.
To enable, follow &lt;a href=&quot;https://mmistakes.github.io/minimal-mistakes/docs/configuration/#utterances-comments&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Note: It will not show comments locally, it will only work once pushed to github on githubpages url.&lt;/p&gt;</content><author><name>{&quot;avatar&quot;=&gt;&quot;/assets/images/bio-photo.jpg&quot;, &quot;bio&quot;=&gt;&quot;Senior Data Engineer&quot;, &quot;links&quot;=&gt;[{&quot;label&quot;=&gt;&quot;Website&quot;, &quot;icon&quot;=&gt;&quot;fas fa-fw fa-link&quot;, &quot;url&quot;=&gt;&quot;https://khuranabal.github.io/&quot;}, {&quot;label&quot;=&gt;&quot;Linkedin&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-linkedin&quot;, &quot;url&quot;=&gt;&quot;https://www.linkedin.com/in/balpreet-singh-654705114&quot;}, {&quot;label&quot;=&gt;&quot;GitHub&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-github&quot;, &quot;url&quot;=&gt;&quot;https://github.com/khuranabal?tab=repositories&quot;}]}</name></author><category term="blog" /><category term="jekyll" /><category term="github" /><summary type="html">To host website we have different ways and for this blog we are focussing on a use case where we need to have a website for blogging, and we are using: jekyll: static site generator, theme used is minimal mistakes github pages: static website hosting service</summary></entry></feed>