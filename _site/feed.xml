<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-01-26T17:04:10+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Balpreet Singh</title><subtitle>Technical data engineering blogs. Techologies: Azure data services like data lake, blob, databricks, database, sql server, terraform, docker, kubernetes Tools: git, visual studio code, azure data studio Languages: SQL, Python, HCL Balpreet Singh, Senior Data Engineer</subtitle><author><name>{&quot;avatar&quot;=&gt;&quot;/assets/images/bio-photo.jpg&quot;, &quot;bio&quot;=&gt;&quot;Senior Data Engineer&quot;, &quot;links&quot;=&gt;[{&quot;label&quot;=&gt;&quot;Website&quot;, &quot;icon&quot;=&gt;&quot;fas fa-fw fa-link&quot;, &quot;url&quot;=&gt;&quot;https://khuranabal.github.io/&quot;}, {&quot;label&quot;=&gt;&quot;Linkedin&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-linkedin&quot;, &quot;url&quot;=&gt;&quot;https://www.linkedin.com/in/balpreet-singh-654705114&quot;}, {&quot;label&quot;=&gt;&quot;GitHub&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-github&quot;, &quot;url&quot;=&gt;&quot;https://github.com/khuranabal?tab=repositories&quot;}]}</name></author><entry><title type="html">cassandra</title><link href="http://localhost:4000/blog/cassandra/" rel="alternate" type="text/html" title="cassandra" /><published>2022-01-26T15:25:00+01:00</published><updated>2022-01-26T15:25:00+01:00</updated><id>http://localhost:4000/blog/cassandra</id><content type="html" xml:base="http://localhost:4000/blog/cassandra/">&lt;h3 id=&quot;features&quot;&gt;features&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;distributed column oriented database&lt;/li&gt;
  &lt;li&gt;highly performant&lt;/li&gt;
  &lt;li&gt;scalable&lt;/li&gt;
  &lt;li&gt;transactional&lt;/li&gt;
  &lt;li&gt;low latency retrieval&lt;/li&gt;
  &lt;li&gt;availibilty and partition tolerance guarentees with tunable consistency&lt;/li&gt;
  &lt;li&gt;no master, multi master&lt;/li&gt;
  &lt;li&gt;decentralized architecure&lt;/li&gt;
  &lt;li&gt;highly available&lt;/li&gt;
  &lt;li&gt;for communication among peers use gossip protocol&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Example: where eventual consistency is fine like social media comments/likes&lt;/p&gt;

&lt;h3 id=&quot;tunable-consistency&quot;&gt;tunable consistency&lt;/h3&gt;

&lt;p&gt;In default setup, it uses eventual consistency. Below are the steps which will be performed when a request is submitted by client to cassandra.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;client send request to get some data&lt;/li&gt;
  &lt;li&gt;request is received by a machine in cassandra cluster lets say node2&lt;/li&gt;
  &lt;li&gt;node2 will get data from the nodes which have data lets say node1&lt;/li&gt;
  &lt;li&gt;node2 will return the response to client&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;But we can tune consistency to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;check all node: all machines in cluster agrees on the same result&lt;/li&gt;
  &lt;li&gt;check quorum: certain number of machine agrees on the same result&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;hbase-vs-cassandra&quot;&gt;hbase vs cassandra&lt;/h3&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;hbase&lt;/td&gt;
      &lt;td&gt;cassandra&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;nosql&lt;/td&gt;
      &lt;td&gt;nosql&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;scalable&lt;/td&gt;
      &lt;td&gt;scalable&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;transactional&lt;/td&gt;
      &lt;td&gt;transactional&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;low latency&lt;/td&gt;
      &lt;td&gt;low latency&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;master slave architecture&lt;/td&gt;
      &lt;td&gt;no single master, muti master&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;CP guarentee&lt;/td&gt;
      &lt;td&gt;AP guarentee, with tunable consistency&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;runs on hadoop cluster&lt;/td&gt;
      &lt;td&gt;dedicated cluster required&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: cassandra has its own query language cql, its similar to sql&lt;/p&gt;</content><author><name>{&quot;avatar&quot;=&gt;&quot;/assets/images/bio-photo.jpg&quot;, &quot;bio&quot;=&gt;&quot;Senior Data Engineer&quot;, &quot;links&quot;=&gt;[{&quot;label&quot;=&gt;&quot;Website&quot;, &quot;icon&quot;=&gt;&quot;fas fa-fw fa-link&quot;, &quot;url&quot;=&gt;&quot;https://khuranabal.github.io/&quot;}, {&quot;label&quot;=&gt;&quot;Linkedin&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-linkedin&quot;, &quot;url&quot;=&gt;&quot;https://www.linkedin.com/in/balpreet-singh-654705114&quot;}, {&quot;label&quot;=&gt;&quot;GitHub&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-github&quot;, &quot;url&quot;=&gt;&quot;https://github.com/khuranabal?tab=repositories&quot;}]}</name></author><category term="blog" /><category term="cassandra" /><category term="nosql" /><category term="hbase" /><category term="database" /><summary type="html">features</summary></entry><entry><title type="html">cap theorem</title><link href="http://localhost:4000/blog/cap/" rel="alternate" type="text/html" title="cap theorem" /><published>2022-01-26T15:10:00+01:00</published><updated>2022-01-26T15:10:00+01:00</updated><id>http://localhost:4000/blog/cap</id><content type="html" xml:base="http://localhost:4000/blog/cap/">&lt;p&gt;It is for distributed databases. And says that we can have only two out of three gurantees.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;C(Consistency): every node returns same recent data&lt;/li&gt;
  &lt;li&gt;A(Availibility): every request gets response&lt;/li&gt;
  &lt;li&gt;P(Partition tolerance): system functions despite of partition/network failure&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Options&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;CA: rdbms&lt;/li&gt;
  &lt;li&gt;CP: single master or active/passive master, distributed database. ex:hbase&lt;/li&gt;
  &lt;li&gt;AP: muti master, distributed database. ex:cassandara, cosmosdb&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: In distributed systems we deal with multiple machine so we would require partition tolerance so the only thing we have to choose is either consistency/availibilty.&lt;/p&gt;

&lt;h3 id=&quot;cp&quot;&gt;CP&lt;/h3&gt;

&lt;p&gt;In this case we can recevice timeout from the application as availibilty will not be guranteed.&lt;/p&gt;

&lt;h3 id=&quot;ap&quot;&gt;AP&lt;/h3&gt;

&lt;p&gt;In this case request receives newest version of data available as consistency will not be guranteed.&lt;/p&gt;</content><author><name>{&quot;avatar&quot;=&gt;&quot;/assets/images/bio-photo.jpg&quot;, &quot;bio&quot;=&gt;&quot;Senior Data Engineer&quot;, &quot;links&quot;=&gt;[{&quot;label&quot;=&gt;&quot;Website&quot;, &quot;icon&quot;=&gt;&quot;fas fa-fw fa-link&quot;, &quot;url&quot;=&gt;&quot;https://khuranabal.github.io/&quot;}, {&quot;label&quot;=&gt;&quot;Linkedin&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-linkedin&quot;, &quot;url&quot;=&gt;&quot;https://www.linkedin.com/in/balpreet-singh-654705114&quot;}, {&quot;label&quot;=&gt;&quot;GitHub&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-github&quot;, &quot;url&quot;=&gt;&quot;https://github.com/khuranabal?tab=repositories&quot;}]}</name></author><category term="blog" /><category term="cap theorem" /><category term="nosql" /><category term="database" /><summary type="html">It is for distributed databases. And says that we can have only two out of three gurantees.</summary></entry><entry><title type="html">hbase</title><link href="http://localhost:4000/blog/hbase/" rel="alternate" type="text/html" title="hbase" /><published>2022-01-21T17:01:00+01:00</published><updated>2022-01-21T17:01:00+01:00</updated><id>http://localhost:4000/blog/hbase</id><content type="html" xml:base="http://localhost:4000/blog/hbase/">&lt;h3 id=&quot;rdbms-properties&quot;&gt;rdbms properties&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;structured&lt;/li&gt;
  &lt;li&gt;random access&lt;/li&gt;
  &lt;li&gt;low latency&lt;/li&gt;
  &lt;li&gt;ACID
    &lt;ul&gt;
      &lt;li&gt;Atomic: full transaction either completes or fail&lt;/li&gt;
      &lt;li&gt;Consistency: updates should not violate constraints&lt;/li&gt;
      &lt;li&gt;Isolation: multiple concurrent sessions can work on database in some sequence using locking&lt;/li&gt;
      &lt;li&gt;Durability: data is stored and recoverable even after failure&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;limitations-of-hadoop&quot;&gt;limitations of hadoop&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;unstructured data&lt;/li&gt;
  &lt;li&gt;no random access&lt;/li&gt;
  &lt;li&gt;high latency&lt;/li&gt;
  &lt;li&gt;no ACID compliance&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;bigtablehbase&quot;&gt;bigtable/hbase&lt;/h3&gt;

&lt;p&gt;bigtable paper is published by google, it is distributed storage system for structured data. hbase is implementation of the same.&lt;/p&gt;

&lt;p&gt;hbase is distributed database management system on top of hadoop. So is:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;distributed&lt;/li&gt;
  &lt;li&gt;scalable&lt;/li&gt;
  &lt;li&gt;fault tolerant&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;hbase-properties&quot;&gt;hbase properties&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;loose structured: as each row can can different number of columns&lt;/li&gt;
  &lt;li&gt;low latency: uses row keys to fetch data&lt;/li&gt;
  &lt;li&gt;random access: uses row keys to fetch data&lt;/li&gt;
  &lt;li&gt;somewhat ACID: as it provides ACID at row level only, not if multiple rows are updated&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It provides batch processing using mapreduce and real time processing using row keys.&lt;/p&gt;

&lt;h3 id=&quot;storage-in-hbase&quot;&gt;storage in hbase&lt;/h3&gt;

&lt;p&gt;It stores data in columnar format as shown below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hbase/hbase.png&quot; alt=&quot;hbase&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Advantages&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;sparse data: In case there is no data for some of the columns then those columns will be skipped while storing&lt;/li&gt;
  &lt;li&gt;dynamic attributes: can have data with more or less columns&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;hbase-vs-rdbms&quot;&gt;hbase vs rdbms&lt;/h3&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;rdbms&lt;/td&gt;
      &lt;td&gt;hbase&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;data is arranged in rows and columns&lt;/td&gt;
      &lt;td&gt;data is in columnar format&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;sql&lt;/td&gt;
      &lt;td&gt;nosql&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;supports groupping, joins, etc&lt;/td&gt;
      &lt;td&gt;supports only CRUD&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;normalized data to reduce redundancy&lt;/td&gt;
      &lt;td&gt;denaormalized (self contained) to reduce disk seek&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ACID compliant&lt;/td&gt;
      &lt;td&gt;ACID compliant at row level&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2 dimiensional (row &amp;amp; column)&lt;/td&gt;
      &lt;td&gt;4 dimensional&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;4-dimensional-model-in-hbase&quot;&gt;4 dimensional model in hbase&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;row key
    &lt;ul&gt;
      &lt;li&gt;uniquely identifies row&lt;/li&gt;
      &lt;li&gt;represented internally as byte array&lt;/li&gt;
      &lt;li&gt;sorted in asc order&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;column family
    &lt;ul&gt;
      &lt;li&gt;all rows have same set of column family&lt;/li&gt;
      &lt;li&gt;setup at definition&lt;/li&gt;
      &lt;li&gt;can have different columns for different rows&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;column
    &lt;ul&gt;
      &lt;li&gt;exist in one of the column family&lt;/li&gt;
      &lt;li&gt;can be dynamic&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;timestamp
    &lt;ul&gt;
      &lt;li&gt;used for versioning&lt;/li&gt;
      &lt;li&gt;vaule for old version can also be accessed&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Below shows all the four dimesnions and how data is stored. Like for col2 all the historic vaules will be stored with the timestamp and by default it gives latest but can be queried old data as well.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hbase/hbase-storage.png&quot; alt=&quot;hbase storage&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;all data is stored in byte array in form of bytes in hbase, there is no concept of data types in it&lt;/li&gt;
  &lt;li&gt;row keys are stored in sorted asc order, so as binary search can be done to find value&lt;/li&gt;
  &lt;li&gt;column families are fix and defined when creating table&lt;/li&gt;
  &lt;li&gt;columns can be more or less&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;hbase-architecture&quot;&gt;hbase architecture&lt;/h3&gt;

&lt;p&gt;hbase table is collection of rows and columns that are sorted based on row key. This table is distributed according to fix size, each portion of table is called region.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hbase/hbase-architecture.png&quot; alt=&quot;hbase architecture&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;region&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;sorted data is stored based on row keys&lt;/li&gt;
  &lt;li&gt;column families are stored in seperate files&lt;/li&gt;
  &lt;li&gt;assume 1 million rows in table and we have 4 regions then .25 million will go in each region&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;region server&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;can have multiple region stored in one region server&lt;/li&gt;
  &lt;li&gt;if 4 data nodes and typically 4 region server is good practice&lt;/li&gt;
  &lt;li&gt;each region server contain wal, block cache&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;wal&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;write ahead log, also known as hlog&lt;/li&gt;
  &lt;li&gt;in case of region server failure, and data is still in memstore. In order to prevent data loss, log is written to permanent store (hdfs)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;block cache&lt;/strong&gt;: whenever read is done data is cached in memory, so next read can be done directly by cache&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;memstore&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;every insert is appended in memstore, after threshold is reached then data is flushed to disk with a new file created and this new file is called hfile&lt;/li&gt;
  &lt;li&gt;for each region there will be memstore per column family&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;zookeeper&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;it is cordinating service for various distributed system&lt;/li&gt;
  &lt;li&gt;hold location for metatable&lt;/li&gt;
  &lt;li&gt;every system send heartbeat&lt;/li&gt;
  &lt;li&gt;metatable hold mapping of rowkey, region and region servers&lt;/li&gt;
  &lt;li&gt;metatable is present on one of region server and zoopkeep knows which region server hold it.&lt;/li&gt;
  &lt;li&gt;in case of hmaster failue, zookeeper assign other master as new master&lt;/li&gt;
  &lt;li&gt;in case region server fails, zookeeper notify hmaster then hmaster reassigns regions of failed region server to other region server&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;hmaster&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;is master and region servers are slaves&lt;/li&gt;
  &lt;li&gt;can have one or more master node but only one will be active at a time&lt;/li&gt;
  &lt;li&gt;assign region servers&lt;/li&gt;
  &lt;li&gt;in case of failure/load it will balance among region servers&lt;/li&gt;
  &lt;li&gt;perform admin stuff like ddl (creating/deleting tables)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;hfiles&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;stored in hdfs&lt;/li&gt;
  &lt;li&gt;stores hbase table data as sorted key-value pairs&lt;/li&gt;
  &lt;li&gt;immutable&lt;/li&gt;
  &lt;li&gt;large in size depends on memstore flush size&lt;/li&gt;
  &lt;li&gt;stores data as set of blocks, which helps reading particular block&lt;/li&gt;
  &lt;li&gt;default block size 64KB&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Example: consider hfile of 250MB and block size of 64KB, then just 64KB need to be scanned if we search for a particular row key, And also in that block, binary search is applied to get particular row as data is stored sorted.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;metatable&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;stores location of regions along with region servers&lt;/li&gt;
  &lt;li&gt;helps user identify the region server and its corresponding regions where specific range data is stored&lt;/li&gt;
  &lt;li&gt;stored in one of region server and its location is stored in zookeeper&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;wal and block cache is one per region server&lt;/li&gt;
  &lt;li&gt;client intreacts directly with the region server to perform read/write operations&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;readwrite-operation&quot;&gt;read/write operation&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hbase/hbase-read-write.png&quot; alt=&quot;hbase read write&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;read&quot;&gt;read&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;region server checks the block cache.&lt;/li&gt;
  &lt;li&gt;if not in block cache them memstore is checked&lt;/li&gt;
  &lt;li&gt;if not in memstore then hfile&lt;/li&gt;
  &lt;li&gt;in hfile particular block will be scanned based on row key and in order to find row key binary search is done in the block&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;write&quot;&gt;write&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;data is first written in wal&lt;/li&gt;
  &lt;li&gt;then to memstore&lt;/li&gt;
  &lt;li&gt;once memstore is filled, content is flushed to hdfs in new hfile&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;compaction&quot;&gt;compaction&lt;/h3&gt;

&lt;p&gt;flush to hfile from memstore create multiple hfiles, especially on frequent write, which leads to slow read. To solve this we can use compactions:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;minor&lt;/strong&gt;: smaller hfiles are processed and rewritten into few larger files&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;major&lt;/strong&gt;: all hfiles are picked and rewritten into single large hfile&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: major compaction is more resource intensive&lt;/p&gt;

&lt;h3 id=&quot;data-updatedelete&quot;&gt;data update/delete&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;update&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;update is done based on timestamp&lt;/li&gt;
  &lt;li&gt;versions are maintained&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;delete&lt;/strong&gt;:
delete is special type of update where vaules of deleted data is not deleted immediatly rather are marked by assigning tombstone marker. Request to read the deleted data then returns null because of tombstone marker which identifyies that data is deleted.&lt;/p&gt;

&lt;p&gt;Reason for this is hfiles are immutable as these are hdfs files. All vaules with tombstone markers are deleted in next major compaction.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: querying hbase has different commands which are not like sql, apache phoenix provides sql like interface on top of hbase.&lt;/p&gt;</content><author><name>{&quot;avatar&quot;=&gt;&quot;/assets/images/bio-photo.jpg&quot;, &quot;bio&quot;=&gt;&quot;Senior Data Engineer&quot;, &quot;links&quot;=&gt;[{&quot;label&quot;=&gt;&quot;Website&quot;, &quot;icon&quot;=&gt;&quot;fas fa-fw fa-link&quot;, &quot;url&quot;=&gt;&quot;https://khuranabal.github.io/&quot;}, {&quot;label&quot;=&gt;&quot;Linkedin&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-linkedin&quot;, &quot;url&quot;=&gt;&quot;https://www.linkedin.com/in/balpreet-singh-654705114&quot;}, {&quot;label&quot;=&gt;&quot;GitHub&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-github&quot;, &quot;url&quot;=&gt;&quot;https://github.com/khuranabal?tab=repositories&quot;}]}</name></author><category term="blog" /><category term="hbase" /><category term="nosql" /><category term="database" /><summary type="html">rdbms properties</summary></entry><entry><title type="html">slowly changing dimensions</title><link href="http://localhost:4000/blog/scd/" rel="alternate" type="text/html" title="slowly changing dimensions" /><published>2022-01-20T16:42:00+01:00</published><updated>2022-01-20T16:42:00+01:00</updated><id>http://localhost:4000/blog/scd</id><content type="html" xml:base="http://localhost:4000/blog/scd/">&lt;p&gt;It is for dimension tables where changes are less in source rdbms which we want to get into datawarehouse or hdfs&lt;/p&gt;

&lt;h3 id=&quot;type-of-scd&quot;&gt;Type of SCD&lt;/h3&gt;

&lt;h4 id=&quot;scd-type-1&quot;&gt;SCD Type 1&lt;/h4&gt;

&lt;p&gt;History is not maintained, data is overwritten.&lt;/p&gt;

&lt;h4 id=&quot;scd-type-2&quot;&gt;SCD Type 2&lt;/h4&gt;

&lt;p&gt;Maintain history. It can be done in below three ways, lets take emaple of product data. All products are stored in products table then:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;version&lt;/strong&gt;: maintain multiple version (increasing int value) of same product latest version is the latest details of the product&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;flag&lt;/strong&gt;: keep flag representing latest details of the product&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;effective date&lt;/strong&gt;: maintain start date and end date, this way we can keep track of the product details over period of time.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>{&quot;avatar&quot;=&gt;&quot;/assets/images/bio-photo.jpg&quot;, &quot;bio&quot;=&gt;&quot;Senior Data Engineer&quot;, &quot;links&quot;=&gt;[{&quot;label&quot;=&gt;&quot;Website&quot;, &quot;icon&quot;=&gt;&quot;fas fa-fw fa-link&quot;, &quot;url&quot;=&gt;&quot;https://khuranabal.github.io/&quot;}, {&quot;label&quot;=&gt;&quot;Linkedin&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-linkedin&quot;, &quot;url&quot;=&gt;&quot;https://www.linkedin.com/in/balpreet-singh-654705114&quot;}, {&quot;label&quot;=&gt;&quot;GitHub&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-github&quot;, &quot;url&quot;=&gt;&quot;https://github.com/khuranabal?tab=repositories&quot;}]}</name></author><category term="blog" /><category term="scd" /><category term="datawarehouse" /><summary type="html">It is for dimension tables where changes are less in source rdbms which we want to get into datawarehouse or hdfs</summary></entry><entry><title type="html">hive features</title><link href="http://localhost:4000/blog/hive-features/" rel="alternate" type="text/html" title="hive features" /><published>2022-01-19T16:43:00+01:00</published><updated>2022-01-19T16:43:00+01:00</updated><id>http://localhost:4000/blog/hive-features</id><content type="html" xml:base="http://localhost:4000/blog/hive-features/">&lt;h3 id=&quot;hive-server--thrift-server&quot;&gt;hive server / thrift server&lt;/h3&gt;

&lt;p&gt;Its like bridge service, where code can be written in any language and executed remotely on hive using apache thrift server/service.&lt;/p&gt;

&lt;p&gt;In hive context, we can write code in java to query on hive using thrift service.&lt;/p&gt;

&lt;h3 id=&quot;msck-repair&quot;&gt;msck repair&lt;/h3&gt;

&lt;p&gt;Assume we have hive external table, and partitions are added to the directory. In this case metadata in hive will not be updated automatically. We have to use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;msck&lt;/code&gt; to update metadata.&lt;/p&gt;

&lt;p&gt;Example:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;create external table
    &lt;pre&gt;&lt;code class=&quot;language-hive&quot;&gt;create external table v1(a int, b int)
partitioned by (state char(2))
location '/data';
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;create directory
    &lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;hadoop fs &lt;span class=&quot;nt&quot;&gt;-mkdir&lt;/span&gt; /data
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;add couple of partitions data in directory &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/data&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;check partitions, it will not show any partitions
    &lt;pre&gt;&lt;code class=&quot;language-hive&quot;&gt;show partitions v1;
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;use below command to build metadata
    &lt;pre&gt;&lt;code class=&quot;language-hive&quot;&gt;msck repair table v1;
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;check partitions again, it will show partitions added
    &lt;pre&gt;&lt;code class=&quot;language-hive&quot;&gt;show partitions v1;
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;no_drop-feature&quot;&gt;no_drop feature&lt;/h3&gt;

&lt;p&gt;We can enable no drop feature so as it is protected by accidental drop command. Same way we can disable if required.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;enable no drop: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;alter table v1 enable no_drop;&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;test drop, it will error out: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;drop table v1;&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;It can even be enabled on particular partition.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;alter table v1 partition (country='DK') enable no_drop;&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;offline-feature&quot;&gt;offline feature&lt;/h3&gt;

&lt;p&gt;We can enable offline feature so as table will be restricted to be queried.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;alter table v1 enable offline;&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;skip-rows&quot;&gt;skip rows&lt;/h3&gt;

&lt;p&gt;To skip n top rows, we have to create table with special tblproperties.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-hive&quot;&gt;create table v2(a int, b int)
tblproperties(&quot;skip.header.line.count&quot;=&quot;3&quot;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note: same way we can skip footer property is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;skip.footer.line.count&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;immutable-feature&quot;&gt;immutable feature&lt;/h3&gt;

&lt;p&gt;If enabled then we cannot change data like append or modify, it only allows data to be loaded one time. However we can overwrite.&lt;/p&gt;

&lt;p&gt;Property to enable: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tblproperties(&quot;immutable&quot;=&quot;true&quot;)&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;drop-vs-truncate-vs-purge&quot;&gt;drop vs truncate vs purge&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;drop&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;for managed table data and metadata both are deleted&lt;/li&gt;
  &lt;li&gt;for external table only metedata is deleted&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;truncate&lt;/strong&gt;: all the data is deleted, metadata will not be deleted&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;purge&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;if set to true, data will be permanently deleted&lt;/li&gt;
  &lt;li&gt;if set to false, data can be recovered&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;empty-string&quot;&gt;empty string&lt;/h3&gt;

&lt;p&gt;When value is missing in the column of dataset in a file, it is empty string &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&quot;&quot;&lt;/code&gt; not &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NULL&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;There is table property &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tblproperties(&quot;serialization.null.format&quot;=&quot;&quot;)&lt;/code&gt; to convert any empty value to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NULL&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;dfs-command-from-hive&quot;&gt;dfs command from hive&lt;/h3&gt;

&lt;p&gt;We can run below command from terminal to check files in hdfs.&lt;/p&gt;
&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;hadoop fs &lt;span class=&quot;nt&quot;&gt;-ls&lt;/span&gt; /user/username
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If we are connected to hive then we can use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dfs&lt;/code&gt;command to check files.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-hive&quot;&gt;dfs -ls /user/username;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;executing-linux-command-from-hive&quot;&gt;executing linux command from hive&lt;/h3&gt;

&lt;p&gt;It can be executed by prefixing command with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;!&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Example: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;!ls /home/user;&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;hivevar&quot;&gt;hivevar&lt;/h3&gt;

&lt;p&gt;It is used to have variable in hive. Example:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-hive&quot;&gt;set hivevar:var1=1;
select * from v1 where a=${var1};
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;print-headers-along-with-data&quot;&gt;print headers along with data&lt;/h3&gt;

&lt;p&gt;Use below to print header in output from hive.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;set hive.cli.prnt.header=true;&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;cartesian-product&quot;&gt;cartesian product&lt;/h3&gt;

&lt;p&gt;Join all rows one table with all rows of other table.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;select * from t1,t2;&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;plan-of-queries&quot;&gt;plan of queries&lt;/h3&gt;

&lt;p&gt;To check plan of queries below can be used:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;EXPLAIN select * from t1;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;EXPLAIN EXTENDED select * from t1;&lt;/code&gt;&lt;/p&gt;</content><author><name>{&quot;avatar&quot;=&gt;&quot;/assets/images/bio-photo.jpg&quot;, &quot;bio&quot;=&gt;&quot;Senior Data Engineer&quot;, &quot;links&quot;=&gt;[{&quot;label&quot;=&gt;&quot;Website&quot;, &quot;icon&quot;=&gt;&quot;fas fa-fw fa-link&quot;, &quot;url&quot;=&gt;&quot;https://khuranabal.github.io/&quot;}, {&quot;label&quot;=&gt;&quot;Linkedin&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-linkedin&quot;, &quot;url&quot;=&gt;&quot;https://www.linkedin.com/in/balpreet-singh-654705114&quot;}, {&quot;label&quot;=&gt;&quot;GitHub&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-github&quot;, &quot;url&quot;=&gt;&quot;https://github.com/khuranabal?tab=repositories&quot;}]}</name></author><category term="blog" /><category term="hive" /><summary type="html">hive server / thrift server</summary></entry><entry><title type="html">hive optimizations</title><link href="http://localhost:4000/blog/hive-optimizations/" rel="alternate" type="text/html" title="hive optimizations" /><published>2022-01-18T16:56:00+01:00</published><updated>2022-01-18T16:56:00+01:00</updated><id>http://localhost:4000/blog/hive-optimizations</id><content type="html" xml:base="http://localhost:4000/blog/hive-optimizations/">&lt;h3 id=&quot;vectorization&quot;&gt;Vectorization&lt;/h3&gt;

&lt;p&gt;It is hive feature which reduces cpu usage for query execution. Usually it processes one row at a time. Vectorized query streamlines operations by processing 1024 rows at a time like bacth of rows.&lt;/p&gt;

&lt;p&gt;To use vectorization data should be in orc format. And need to enable below parameter, it is not enabled by default.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;set hive.vectorized.execution.enabled = true&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Example:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;create table: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;create table v1(a int, b int) stored as orc;&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;enable vectorization&lt;/li&gt;
  &lt;li&gt;expalin query &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;explain select * from v1;&lt;/code&gt; We should be able to see &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;execution mode: vectorized&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;change-hive-engine&quot;&gt;Change hive engine&lt;/h3&gt;

&lt;p&gt;It supports three engines. We can check which one is in use by below command.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;set hive.execution.engine&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;set hive.execution.engine=spark;&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;mr&lt;/li&gt;
  &lt;li&gt;tez&lt;/li&gt;
  &lt;li&gt;spark&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: mr is defult selected&lt;/p&gt;

&lt;h3 id=&quot;udf&quot;&gt;UDF&lt;/h3&gt;

&lt;p&gt;UDFâ€™s are not very optimized. And filters in hive are evaluated from left to right so in case we have udf then we should put that in last in the filter. Example:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;where col1=1 and udf_name(col2)=2&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;cost-based-optimization-cbo&quot;&gt;cost based optimization (CBO)&lt;/h3&gt;

&lt;p&gt;It is open source and generates effecient plan by checking cost of query, which is collected by ANALYZE statements. And it is enabled by default. Properties related to it:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-hive&quot;&gt;set hive.cbo.enable
set hive.compute.query.using.stats
set hive.stats.fetch.column.stats
set hive.stats.fetch.partition.stats
&lt;/code&gt;&lt;/pre&gt;</content><author><name>{&quot;avatar&quot;=&gt;&quot;/assets/images/bio-photo.jpg&quot;, &quot;bio&quot;=&gt;&quot;Senior Data Engineer&quot;, &quot;links&quot;=&gt;[{&quot;label&quot;=&gt;&quot;Website&quot;, &quot;icon&quot;=&gt;&quot;fas fa-fw fa-link&quot;, &quot;url&quot;=&gt;&quot;https://khuranabal.github.io/&quot;}, {&quot;label&quot;=&gt;&quot;Linkedin&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-linkedin&quot;, &quot;url&quot;=&gt;&quot;https://www.linkedin.com/in/balpreet-singh-654705114&quot;}, {&quot;label&quot;=&gt;&quot;GitHub&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-github&quot;, &quot;url&quot;=&gt;&quot;https://github.com/khuranabal?tab=repositories&quot;}]}</name></author><category term="blog" /><category term="hive" /><category term="optimization" /><summary type="html">Vectorization</summary></entry><entry><title type="html">compression in hadoop</title><link href="http://localhost:4000/blog/compression-in-hadoop/" rel="alternate" type="text/html" title="compression in hadoop" /><published>2022-01-18T16:29:00+01:00</published><updated>2022-01-18T16:29:00+01:00</updated><id>http://localhost:4000/blog/compression-in-hadoop</id><content type="html" xml:base="http://localhost:4000/blog/compression-in-hadoop/">&lt;p&gt;Compression will help to:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;save storage&lt;/li&gt;
  &lt;li&gt;reduce io cost&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: compression and uncompression adds some cost as cpu resources will be used but io cost is saved more comparatively.&lt;/p&gt;

&lt;h3 id=&quot;compression-techniques&quot;&gt;Compression techniques&lt;/h3&gt;

&lt;p&gt;some compression codes are optimized for:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;storage&lt;/li&gt;
  &lt;li&gt;speed&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;snappy&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;fast compression codec&lt;/li&gt;
  &lt;li&gt;optimized for speed rather than storage&lt;/li&gt;
  &lt;li&gt;by default is not splittable but file format like avro/orc/parquet takes care of splits. So snappy can be used with these file formats.&lt;/li&gt;
  &lt;li&gt;distributed with hadoop&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;lzo&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;optimized for speed rather than storage&lt;/li&gt;
  &lt;li&gt;it is splittable but requires additional indexing step&lt;/li&gt;
  &lt;li&gt;good for plain text files&lt;/li&gt;
  &lt;li&gt;is not distributed with hadoop, requires seperate install&lt;/li&gt;
  &lt;li&gt;compratively slower than snappy&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;gzip&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;optimized for storage, 2.5x times compression compared to snappy&lt;/li&gt;
  &lt;li&gt;not splittable but can be used with container file formats like snappy&lt;/li&gt;
  &lt;li&gt;processing is slow, as compression is more it will have less blocks, can reduce block size to increase parellelism which will process faster&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;bzip2&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;excellent storage, compress around 9% more comparitively to gzip&lt;/li&gt;
  &lt;li&gt;significantly slower, around 10x comparitively to gzip&lt;/li&gt;
  &lt;li&gt;splittable&lt;/li&gt;
  &lt;li&gt;might be used for archival&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: snappy is used in mostly, as it provides good trade off between speed and size.&lt;/p&gt;</content><author><name>{&quot;avatar&quot;=&gt;&quot;/assets/images/bio-photo.jpg&quot;, &quot;bio&quot;=&gt;&quot;Senior Data Engineer&quot;, &quot;links&quot;=&gt;[{&quot;label&quot;=&gt;&quot;Website&quot;, &quot;icon&quot;=&gt;&quot;fas fa-fw fa-link&quot;, &quot;url&quot;=&gt;&quot;https://khuranabal.github.io/&quot;}, {&quot;label&quot;=&gt;&quot;Linkedin&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-linkedin&quot;, &quot;url&quot;=&gt;&quot;https://www.linkedin.com/in/balpreet-singh-654705114&quot;}, {&quot;label&quot;=&gt;&quot;GitHub&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-github&quot;, &quot;url&quot;=&gt;&quot;https://github.com/khuranabal?tab=repositories&quot;}]}</name></author><category term="blog" /><category term="compression" /><category term="hadoop" /><summary type="html">Compression will help to: save storage reduce io cost</summary></entry><entry><title type="html">file formats for big data</title><link href="http://localhost:4000/blog/file-formats/" rel="alternate" type="text/html" title="file formats for big data" /><published>2022-01-14T16:33:00+01:00</published><updated>2022-01-14T16:33:00+01:00</updated><id>http://localhost:4000/blog/file-formats</id><content type="html" xml:base="http://localhost:4000/blog/file-formats/">&lt;p&gt;There are certain parameters to consider when chossing a file format.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;storage space consumption&lt;/li&gt;
  &lt;li&gt;processing time&lt;/li&gt;
  &lt;li&gt;io consumption&lt;/li&gt;
  &lt;li&gt;read/write speed&lt;/li&gt;
  &lt;li&gt;if data can be split in files&lt;/li&gt;
  &lt;li&gt;schema evolution&lt;/li&gt;
  &lt;li&gt;compression&lt;/li&gt;
  &lt;li&gt;compatible with framework like hive/spark etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;ways-in-which-data-can-be-stored&quot;&gt;ways in which data can be stored&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;row based&lt;/strong&gt;: write is simple but read will have to read full row even if subset of column is read.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;column based&lt;/strong&gt;: all columns values are stored together. Write will be slower comparatively. But read will be efficient. In this we can get good compression as well.&lt;/p&gt;

&lt;h3 id=&quot;file-formats&quot;&gt;file formats&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;csv&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;all data stored as text so takes a lot of storage for example if column has integer value then that consumes more storage because its stored as text.&lt;/li&gt;
  &lt;li&gt;processing will be slow as conversion need to be done.&lt;/li&gt;
  &lt;li&gt;io will be slow as data storage is more so will do more io.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;xml&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;semi structure&lt;/li&gt;
  &lt;li&gt;all negative of csv applies here as well.&lt;/li&gt;
  &lt;li&gt;these files can not split.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;json&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;semi structure&lt;/li&gt;
  &lt;li&gt;all negative of csv applies here as well.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;avro&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;row based storage&lt;/li&gt;
  &lt;li&gt;faster write as its row based&lt;/li&gt;
  &lt;li&gt;slow read for subset of columns&lt;/li&gt;
  &lt;li&gt;schema of file is stored in json&lt;/li&gt;
  &lt;li&gt;data is self describing because schema is embeded as part of data&lt;/li&gt;
  &lt;li&gt;actual data is in binary format&lt;/li&gt;
  &lt;li&gt;general file format, programming language agnostic can used in many languages&lt;/li&gt;
  &lt;li&gt;matuare in schema evolution&lt;/li&gt;
  &lt;li&gt;serialization format&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;orc&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;optimized row columnar&lt;/li&gt;
  &lt;li&gt;write are not effecient&lt;/li&gt;
  &lt;li&gt;effecient reads&lt;/li&gt;
  &lt;li&gt;highly effecient in terms of storage&lt;/li&gt;
  &lt;li&gt;compression (dictionary encoding, bit packing, delta encoding, run length encoding along with generalzed compression like snappy/gzip)&lt;/li&gt;
  &lt;li&gt;predicate pushdown&lt;/li&gt;
  &lt;li&gt;best fit for hive, supports all datatypes including complex used in hive&lt;/li&gt;
  &lt;li&gt;initially specially designed for hive&lt;/li&gt;
  &lt;li&gt;supports schema evolution, not matuare as avro&lt;/li&gt;
  &lt;li&gt;self describing, as stores metadata(using protocol buffers) in the end of file itself&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;parquet&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;column based storage&lt;/li&gt;
  &lt;li&gt;writes are not effecient&lt;/li&gt;
  &lt;li&gt;effecient reads&lt;/li&gt;
  &lt;li&gt;shares many design patterns as of orc, but more general purpose&lt;/li&gt;
  &lt;li&gt;very good in handling nested data&lt;/li&gt;
  &lt;li&gt;compression is effecient&lt;/li&gt;
  &lt;li&gt;self describing, as stores metadata in the end of file itself&lt;/li&gt;
  &lt;li&gt;supports schema evolution adding/removing columns in the end&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;orc-storage-internals&quot;&gt;orc storage internals&lt;/h3&gt;

&lt;p&gt;Data is stored as shown in the below image. Mainly it has below sections.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Header&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;It contains text &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ORC&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Body&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In it data is divided in multiple stripes (default size is 250MB) and each stripe has-&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Index data: max, min, count of each column in every row group in the stripe&lt;/li&gt;
  &lt;li&gt;Row data: data is broken in row groups each row group has 10000 rows by default&lt;/li&gt;
  &lt;li&gt;Stripe footer: stores encoding used&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Footer&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;File footer: contains metadata at file and stripe level like max, min, count.&lt;/li&gt;
  &lt;li&gt;Postscript: stores which compression is used like snappy/gzip, postscipt is never compressed&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/fileformats/orc.png&quot; alt=&quot;orc internals&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Flow is like header is read to identify orc file and then postscript is read to get compression used and then file footer then stripes and row data.&lt;/p&gt;

&lt;h3 id=&quot;parquet-storage-internals&quot;&gt;parquet storage internals&lt;/h3&gt;

&lt;p&gt;Data is stored as shown in the below image. Mainly it has below sections.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Header&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;It contains text &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PAR1&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Row group&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In it data is divided in column chunks which is further divided in pages.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Footer&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;File metadata: encoding, schema, etc.&lt;/li&gt;
  &lt;li&gt;Lenght of file metadata&lt;/li&gt;
  &lt;li&gt;Magic number &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PAR1&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/fileformats/parquet.png&quot; alt=&quot;parquet internals&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;compression&quot;&gt;compression&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;dictionary encoding&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Suppose we have sales data where product name column, customer name column exist then that will have same values for a lot of rows. For this dictonary encoding helps by having dictionary where distinct vaules are stored and then referenced.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;bit packing&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Suppose we have int column in the dateset, for that column if the vaule is less than bit packing can help by representing same number with less bits.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;delta encoding&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Suppose we have timestamp column in dataset then first timestamp is stored and then for next column vaule it can store difference only. Example vaule is 123456 and next vaule is 123457 then base vaule stored can be 123456 and next value 1.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;run length encoding&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Suppose we have column in dataset which has value dddddfffgg then the vaule stored is d5f3g2&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;serialization is converting data into a form which can be easily transferred over the network and stored in file system.&lt;/li&gt;
  &lt;li&gt;there is no other file format better than avro for schema evolution.&lt;/li&gt;
  &lt;li&gt;avro can be best fit for landing raw data in data lake.&lt;/li&gt;
  &lt;li&gt;in avro, orc, parquet any compression can be used, compression code is stored in metadata, so reader can get to know compression code from metadata.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;sources&quot;&gt;Sources&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;https://orc.apache.org/specification/ORCv2/&lt;/li&gt;
  &lt;li&gt;https://parquet.apache.org/documentation/latest/&lt;/li&gt;
&lt;/ul&gt;</content><author><name>{&quot;avatar&quot;=&gt;&quot;/assets/images/bio-photo.jpg&quot;, &quot;bio&quot;=&gt;&quot;Senior Data Engineer&quot;, &quot;links&quot;=&gt;[{&quot;label&quot;=&gt;&quot;Website&quot;, &quot;icon&quot;=&gt;&quot;fas fa-fw fa-link&quot;, &quot;url&quot;=&gt;&quot;https://khuranabal.github.io/&quot;}, {&quot;label&quot;=&gt;&quot;Linkedin&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-linkedin&quot;, &quot;url&quot;=&gt;&quot;https://www.linkedin.com/in/balpreet-singh-654705114&quot;}, {&quot;label&quot;=&gt;&quot;GitHub&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-github&quot;, &quot;url&quot;=&gt;&quot;https://github.com/khuranabal?tab=repositories&quot;}]}</name></author><category term="blog" /><category term="csv" /><category term="xml" /><category term="json" /><category term="avro" /><category term="orc" /><category term="parquet" /><category term="compression" /><summary type="html">There are certain parameters to consider when chossing a file format.</summary></entry><entry><title type="html">sqoop working</title><link href="http://localhost:4000/blog/sqoop/" rel="alternate" type="text/html" title="sqoop working" /><published>2022-01-12T17:16:00+01:00</published><updated>2022-01-12T17:16:00+01:00</updated><id>http://localhost:4000/blog/sqoop</id><content type="html" xml:base="http://localhost:4000/blog/sqoop/">&lt;p&gt;It is used to transfer data between rdbms to hdfs and vice versa.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;sqoop import: transfer from rdbms to hdfs&lt;/li&gt;
  &lt;li&gt;sqoop export: transfer from hdfs to rdbms&lt;/li&gt;
  &lt;li&gt;sqoop eval: to run queries on databases&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;setup-cloudera-quickstart-machine-on-local&quot;&gt;Setup cloudera quickstart machine on local&lt;/h2&gt;
&lt;p&gt;We have multiple ways to setup cloudera machine either as VM or docker image, in this case we will use docker and local machine is ubuntu. So, prerequisite is to:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;setup docker, follow &lt;a href=&quot;https://docs.docker.com/engine/install/ubuntu/&quot;&gt;here&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;run below to pull cloudera quickstart image and then run image which will setup cloudera machine to interact via terminal&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker pull cloudera/quickstart:latest
docker run &lt;span class=&quot;nt&quot;&gt;--hostname&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;quickstart.cloudera &lt;span class=&quot;nt&quot;&gt;--privileged&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-i&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 8888:8888 &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 80:80 &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 8080:8080 cloudera/quickstart /usr/bin/docker-quickstart
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;commands&quot;&gt;Commands&lt;/h2&gt;

&lt;h3 id=&quot;list-databases&quot;&gt;list databases&lt;/h3&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sqoop-list-databases &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--connect&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;jdbc:mysql://quickstart.cloudera:3306&quot;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--username&lt;/span&gt; root &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--password&lt;/span&gt; cloudera
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;list-tables&quot;&gt;list tables&lt;/h3&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sqoop-list-tables &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--connect&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;jdbc:mysql://quickstart.cloudera:3306/retail_db&quot;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--username&lt;/span&gt; root &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--password&lt;/span&gt; cloudera
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;query-database&quot;&gt;query database&lt;/h3&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sqoop-eval &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--connect&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;jdbc:mysql://quickstart.cloudera:3306&quot;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--username&lt;/span&gt; root &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--password&lt;/span&gt; cloudera &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--query&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;select * from retail_db.customers limit 5&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;import-one-table&quot;&gt;import one table&lt;/h3&gt;
&lt;p&gt;It will run mapreduce job where only mappers will work. Default four mappers will run but can be changed. It divide work based on primary key. If no primary key then command will failm in that case we have two options:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;change number of mapper to one: use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-m 1&lt;/code&gt; parameter&lt;/li&gt;
  &lt;li&gt;use split by column.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Example: Here &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;orders&lt;/code&gt; table has primary key and we testing import on this table.&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sqoop import &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--connect&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;jdbc:mysql://quickstart.cloudera:3306/retail_db&quot;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--username&lt;/span&gt; root &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--password&lt;/span&gt; cloudera &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--table&lt;/span&gt; orders &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--target-dir&lt;/span&gt; /result
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Run below command after above import command to check files produced by import command, default number of mappers are 4 so thats why it will have four part-m files.&lt;/p&gt;
&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;hadoop fs &lt;span class=&quot;nt&quot;&gt;-ls&lt;/span&gt; /result
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;import-all-tables&quot;&gt;import all tables&lt;/h3&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sqoop-import-all-tables &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--connect&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;jdbc:mysql://quickstart.cloudera:3306/retail_db&quot;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--username&lt;/span&gt; root &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--password&lt;/span&gt; cloudera &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--table&lt;/span&gt; orders &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--warehouse-dir&lt;/span&gt; /result
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;target-dir: files will directly be stored in the directory mentioned in command.&lt;/li&gt;
  &lt;li&gt;warehouse-dir: files will be stored in the subdirectory (same as tablename) of the directory mentioned in command.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note: sqoop is retired as of June 2021&lt;/p&gt;

&lt;p&gt;Sources:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Cloudera quickstart docker image for testing, &lt;a href=&quot;https://hub.docker.com/r/cloudera/quickstart/&quot;&gt;source&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>{&quot;avatar&quot;=&gt;&quot;/assets/images/bio-photo.jpg&quot;, &quot;bio&quot;=&gt;&quot;Senior Data Engineer&quot;, &quot;links&quot;=&gt;[{&quot;label&quot;=&gt;&quot;Website&quot;, &quot;icon&quot;=&gt;&quot;fas fa-fw fa-link&quot;, &quot;url&quot;=&gt;&quot;https://khuranabal.github.io/&quot;}, {&quot;label&quot;=&gt;&quot;Linkedin&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-linkedin&quot;, &quot;url&quot;=&gt;&quot;https://www.linkedin.com/in/balpreet-singh-654705114&quot;}, {&quot;label&quot;=&gt;&quot;GitHub&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-github&quot;, &quot;url&quot;=&gt;&quot;https://github.com/khuranabal?tab=repositories&quot;}]}</name></author><category term="blog" /><category term="sqoop" /><category term="hdfs" /><summary type="html">It is used to transfer data between rdbms to hdfs and vice versa.</summary></entry><entry><title type="html">MapReduce working</title><link href="http://localhost:4000/blog/map-reduce/" rel="alternate" type="text/html" title="MapReduce working" /><published>2022-01-11T15:13:00+01:00</published><updated>2022-01-11T15:13:00+01:00</updated><id>http://localhost:4000/blog/map-reduce</id><content type="html" xml:base="http://localhost:4000/blog/map-reduce/">&lt;p&gt;MapReduce is programming model for processing big datsets. It consists of two stages:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;map&lt;/li&gt;
  &lt;li&gt;reduce&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Both works on key value pairs.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;key&lt;/td&gt;
      &lt;td&gt;value&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;id&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;name&lt;/td&gt;
      &lt;td&gt;abc&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/mapreduce/mapreduce-input-output.png&quot; alt=&quot;mapreduce input output&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;understanding-with-example&quot;&gt;Understanding with example&lt;/h3&gt;

&lt;p&gt;Lets say we have a big file (size 100GB) and we need to find frequency of words.
By default block size is 128MB so data will be divided as per block size and distributed among nodes. Considering with default one reducer.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/mapreduce/sample-input-output.png&quot; alt=&quot;sample data&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Clearly the input is not in key value format, so here record reader is used to read input file and convert each line in key vaule pair. Here each line will be assigned a key by record reader and the vaule will be the actual line content.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Mapper logic&lt;/strong&gt;: Just take the vaules, and split on space to find words and assign word as key and 1 as vaule.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Shuffle &amp;amp; Sort&lt;/strong&gt;: Transfer all the key vaule pairs on to single machine (reduer). Sorting will happen on keys, for example output will be in format: (key, {vaule, vaule, â€¦})&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Reducer logic&lt;/strong&gt;: Iterate over list of vaules in a key and sum it up.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/mapreduce/processing.png&quot; alt=&quot;processing&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Number of mappers will be equal to number of data blocks.&lt;/li&gt;
  &lt;li&gt;Number of reducer is set by developer default is 1 but we can set to 0 or any higher number.&lt;/li&gt;
  &lt;li&gt;System take care of shuffle and sort operation.&lt;/li&gt;
  &lt;li&gt;Record reader works on mapper machine.&lt;/li&gt;
  &lt;li&gt;If number of reducers are 0 then shuffle and sort will not execute.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;optimization&quot;&gt;Optimization&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;More work on mappers&lt;/strong&gt;: It will optimize the processing as all mappers are executed parellely. More on this below in combiner section.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Increase reducer&lt;/strong&gt;: When we cannot put more work in the mapper then we can think of increasing reducer. Example: if all mappers take 1 min and reducer takes 4 min then we can consider increasing number of reducer in order to optimize the run time.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;No reducer&lt;/strong&gt;: Where no aggregation required, example: filtering.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;No shuffle and sort will be executed if reducer is not used.&lt;/li&gt;
  &lt;li&gt;If we have more than one reducer then output of each mapper is divided in n partitions if n reducers are in cluster, partition 1 from each mapper goes to reducer 1 and so on. Hash function is used to route which key value goes to which partition. Hash function is consistent so as all same keys are in the same reducer.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;combiner&quot;&gt;Combiner&lt;/h4&gt;

&lt;p&gt;In some cases, when work is moved more on the mapper that means we can have some aggreation work on mapper. Like if we need max value of the key then that can be performed on mapper as well. And the aggregation which is done on mapper is done by combiner process, it is executed on mapper machines itself.&lt;/p&gt;

&lt;p&gt;But avoid using in case where data can change when executing on one machine compared to multiple machine like avg.&lt;/p&gt;

&lt;p&gt;Note: Combiner logic is same as reducer logic in this case for example: max, min, sum.&lt;/p&gt;

&lt;h3 id=&quot;flow-of-data&quot;&gt;Flow of data&lt;/h3&gt;

&lt;p&gt;record reader -&amp;gt; mapper -&amp;gt; combiner -&amp;gt; partitioner -&amp;gt; shuffle &amp;amp; sort -&amp;gt; reducer&lt;/p&gt;</content><author><name>{&quot;avatar&quot;=&gt;&quot;/assets/images/bio-photo.jpg&quot;, &quot;bio&quot;=&gt;&quot;Senior Data Engineer&quot;, &quot;links&quot;=&gt;[{&quot;label&quot;=&gt;&quot;Website&quot;, &quot;icon&quot;=&gt;&quot;fas fa-fw fa-link&quot;, &quot;url&quot;=&gt;&quot;https://khuranabal.github.io/&quot;}, {&quot;label&quot;=&gt;&quot;Linkedin&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-linkedin&quot;, &quot;url&quot;=&gt;&quot;https://www.linkedin.com/in/balpreet-singh-654705114&quot;}, {&quot;label&quot;=&gt;&quot;GitHub&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-github&quot;, &quot;url&quot;=&gt;&quot;https://github.com/khuranabal?tab=repositories&quot;}]}</name></author><category term="blog" /><category term="mapreduce" /><category term="hadoop" /><summary type="html">MapReduce is programming model for processing big datsets. It consists of two stages: map reduce</summary></entry></feed>