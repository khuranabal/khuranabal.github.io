<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-01-12T18:39:19+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Balpreet Singh</title><subtitle>Data Engineer blogs. Azure data services like data lake, blob, databricks, database, sql server, terraform, docker, kubernetes, SQL, Python, HCL</subtitle><author><name>{&quot;avatar&quot;=&gt;&quot;/assets/images/bio-photo.jpg&quot;, &quot;bio&quot;=&gt;&quot;Senior Data Engineer&quot;, &quot;links&quot;=&gt;[{&quot;label&quot;=&gt;&quot;Website&quot;, &quot;icon&quot;=&gt;&quot;fas fa-fw fa-link&quot;, &quot;url&quot;=&gt;&quot;https://khuranabal.github.io/&quot;}, {&quot;label&quot;=&gt;&quot;Linkedin&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-linkedin&quot;, &quot;url&quot;=&gt;&quot;https://www.linkedin.com/in/balpreet-singh-654705114&quot;}, {&quot;label&quot;=&gt;&quot;GitHub&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-github&quot;, &quot;url&quot;=&gt;&quot;https://github.com/khuranabal?tab=repositories&quot;}]}</name></author><entry><title type="html">sqoop working</title><link href="http://localhost:4000/blog/sqoop/" rel="alternate" type="text/html" title="sqoop working" /><published>2022-01-12T17:16:00+01:00</published><updated>2022-01-12T17:16:00+01:00</updated><id>http://localhost:4000/blog/sqoop</id><content type="html" xml:base="http://localhost:4000/blog/sqoop/">&lt;p&gt;It is used to transfer data between rdbms to hdfs and vice versa.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;sqoop import: transfer from rdbms to hdfs&lt;/li&gt;
  &lt;li&gt;sqoop export: transfer from hdfs to rdbms&lt;/li&gt;
  &lt;li&gt;sqoop eval: to run queries on databases&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;commands&quot;&gt;Commands&lt;/h2&gt;

&lt;h3 id=&quot;list-databases&quot;&gt;list databases&lt;/h3&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sqoop-list-databases &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--connect&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;jdbc:mysql://quickstart.cloudera:3306&quot;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--username&lt;/span&gt; root &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--password&lt;/span&gt; cloudera
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;list-tables&quot;&gt;list tables&lt;/h3&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sqoop-list-tables &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--connect&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;jdbc:mysql://quickstart.cloudera:3306&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\r&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;etail_db&quot;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--username&lt;/span&gt; root &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--password&lt;/span&gt; cloudera
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;query-database&quot;&gt;query database&lt;/h3&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sqoop-eval &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--connect&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;jdbc:mysql://quickstart.cloudera:3306&quot;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--username&lt;/span&gt; root &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--password&lt;/span&gt; cloudera &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--query&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;select * from retail_db.customers limit 5&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Sources: Cloudera quickstart VM or docker image for testing.&lt;/p&gt;</content><author><name>{&quot;avatar&quot;=&gt;&quot;/assets/images/bio-photo.jpg&quot;, &quot;bio&quot;=&gt;&quot;Senior Data Engineer&quot;, &quot;links&quot;=&gt;[{&quot;label&quot;=&gt;&quot;Website&quot;, &quot;icon&quot;=&gt;&quot;fas fa-fw fa-link&quot;, &quot;url&quot;=&gt;&quot;https://khuranabal.github.io/&quot;}, {&quot;label&quot;=&gt;&quot;Linkedin&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-linkedin&quot;, &quot;url&quot;=&gt;&quot;https://www.linkedin.com/in/balpreet-singh-654705114&quot;}, {&quot;label&quot;=&gt;&quot;GitHub&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-github&quot;, &quot;url&quot;=&gt;&quot;https://github.com/khuranabal?tab=repositories&quot;}]}</name></author><category term="blog" /><category term="sqoop" /><summary type="html">It is used to transfer data between rdbms to hdfs and vice versa.</summary></entry><entry><title type="html">MapReduce working</title><link href="http://localhost:4000/blog/map-reduce/" rel="alternate" type="text/html" title="MapReduce working" /><published>2022-01-11T15:13:00+01:00</published><updated>2022-01-11T15:13:00+01:00</updated><id>http://localhost:4000/blog/map-reduce</id><content type="html" xml:base="http://localhost:4000/blog/map-reduce/">&lt;p&gt;MapReduce is programming model for processing big datsets. It consists of two stages:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;map&lt;/li&gt;
  &lt;li&gt;reduce&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Both works on key value pairs.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;key&lt;/td&gt;
      &lt;td&gt;value&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;id&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;name&lt;/td&gt;
      &lt;td&gt;abc&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/mapreduce/mapreduce-input-output.png&quot; alt=&quot;mapreduce input output&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;understanding-with-example&quot;&gt;Understanding with example&lt;/h3&gt;

&lt;p&gt;Lets say we have a big file (size 100GB) and we need to find frequency of words.
By default block size is 128MB so data will be divided as per block size and distributed among nodes. Considering with default one reducer.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/mapreduce/sample-input-output.png&quot; alt=&quot;sample data&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Clearly the input is not in key value format, so here record reader is used to read input file and convert each line in key vaule pair. Here each line will be assigned a key by record reader and the vaule will be the actual line content.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Mapper logic&lt;/strong&gt;: Just take the vaules, and split on space to find words and assign word as key and 1 as vaule.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Shuffle &amp;amp; Sort&lt;/strong&gt;: Transfer all the key vaule pairs on to single machine (reduer). Sorting will happen on keys, for example output will be in format: (key, {vaule, vaule, …})&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Reducer logic&lt;/strong&gt;: Iterate over list of vaules in a key and sum it up.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/mapreduce/processing.png&quot; alt=&quot;processing&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Number of mappers will be equal to number of data blocks.&lt;/li&gt;
  &lt;li&gt;Number of reducer is set by developer default is 1 but we can set to 0 or any higher number.&lt;/li&gt;
  &lt;li&gt;System take care of shuffle and sort operation.&lt;/li&gt;
  &lt;li&gt;Record reader works on mapper machine.&lt;/li&gt;
  &lt;li&gt;If number of reducers are 0 then shuffle and sort will not execute.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;optimization&quot;&gt;Optimization&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;More work on mappers&lt;/strong&gt;: It will optimize the processing as all mappers are executed parellely. More on this below in combiner section.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Increase reducer&lt;/strong&gt;: When we cannot put more work in the mapper then we can think of increasing reducer. Example: if all mappers take 1 min and reducer takes 4 min then we can consider increasing number of reducer in order to optimize the run time.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;No reducer&lt;/strong&gt;: Where no aggregation required, example: filtering.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;No shuffle and sort will be executed if reducer is not used.&lt;/li&gt;
  &lt;li&gt;If we have more than one reducer then output of each mapper is divided in n partitions if n reducers are in cluster, partition 1 from each mapper goes to reducer 1 and so on. Hash function is used to route which key value goes to which partition. Hash function is consistent so as all same keys are in the same reducer.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;combiner&quot;&gt;Combiner&lt;/h4&gt;

&lt;p&gt;In some cases, when work is moved more on the mapper that means we can have some aggreation work on mapper. Like if we need max value of the key then that can be performed on mapper as well. And the aggregation which is done on mapper is done by combiner process, it is executed on mapper machines itself.&lt;/p&gt;

&lt;p&gt;But avoid using in case where data can change when executing on one machine compared to multiple machine like avg.&lt;/p&gt;

&lt;p&gt;Note: Combiner logic is same as reducer logic in this case for example: max, min, sum.&lt;/p&gt;

&lt;h3 id=&quot;flow-of-data&quot;&gt;Flow of data&lt;/h3&gt;

&lt;p&gt;record reader -&amp;gt; mapper -&amp;gt; combiner -&amp;gt; partitioner -&amp;gt; shuffle &amp;amp; sort -&amp;gt; reducer&lt;/p&gt;</content><author><name>{&quot;avatar&quot;=&gt;&quot;/assets/images/bio-photo.jpg&quot;, &quot;bio&quot;=&gt;&quot;Senior Data Engineer&quot;, &quot;links&quot;=&gt;[{&quot;label&quot;=&gt;&quot;Website&quot;, &quot;icon&quot;=&gt;&quot;fas fa-fw fa-link&quot;, &quot;url&quot;=&gt;&quot;https://khuranabal.github.io/&quot;}, {&quot;label&quot;=&gt;&quot;Linkedin&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-linkedin&quot;, &quot;url&quot;=&gt;&quot;https://www.linkedin.com/in/balpreet-singh-654705114&quot;}, {&quot;label&quot;=&gt;&quot;GitHub&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-github&quot;, &quot;url&quot;=&gt;&quot;https://github.com/khuranabal?tab=repositories&quot;}]}</name></author><category term="blog" /><category term="mapreduce" /><summary type="html">MapReduce is programming model for processing big datsets. It consists of two stages: map reduce</summary></entry><entry><title type="html">hdfs architecture</title><link href="http://localhost:4000/blog/understand-hdfs/" rel="alternate" type="text/html" title="hdfs architecture" /><published>2022-01-08T14:45:00+01:00</published><updated>2022-01-08T14:45:00+01:00</updated><id>http://localhost:4000/blog/understand-hdfs</id><content type="html" xml:base="http://localhost:4000/blog/understand-hdfs/">&lt;p&gt;hdfs is hadoop distributed file system. Highly fault tolerant and is designed to deploy on low cost machines.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hdfs/hdfs-architecture.png&quot; alt=&quot;hdfs architecture&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let’s assume we have cluster(2 data node, 1 name node) and file of 250MB.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Default block size: 128MB&lt;/li&gt;
  &lt;li&gt;Default replication factor: 3&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So, there will be 2 blocks:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Block-1: 128MB&lt;/li&gt;
  &lt;li&gt;Block-2: 122MB&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When data will be loaded then both the blocks will be loaded on different data node, behind the scene as per default setting each block will be replicated 3 times, in this case 1 node will have 2 copies of block and other node will have 1 copy of block.&lt;/p&gt;

&lt;p&gt;Note:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;last block might have less than 128MB depending on the size of file, but space is not wasted as remaining space will free up and is used by other blocks.&lt;/li&gt;
  &lt;li&gt;one data block cannot be shared by two files, so last block of one file is not used by other file bock&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;metadata-stored-in-name-node&quot;&gt;Metadata stored in name node&lt;/h3&gt;
&lt;p&gt;At high level below details are stored in metadata.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Filename&lt;/li&gt;
  &lt;li&gt;Block&lt;/li&gt;
  &lt;li&gt;Data node: All the data nodes which have the block&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Example:&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;File name&lt;/td&gt;
      &lt;td&gt;Block Number&lt;/td&gt;
      &lt;td&gt;Data node 1&lt;/td&gt;
      &lt;td&gt;Data Node 2&lt;/td&gt;
      &lt;td&gt;Data Node 3&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;data-node-failure&quot;&gt;Data node failure&lt;/h3&gt;
&lt;p&gt;In case of any failue in data node other data node will be responsible for serving requests and name node will maintain the copies as per replication factor.&lt;/p&gt;

&lt;h4 id=&quot;1-dead-data-node&quot;&gt;1. Dead data node&lt;/h4&gt;
&lt;p&gt;Each data node sends heart beat every 3 seconds to name node and if 10 consecutive times hear beat is not received from a data node then data node is treated as dead or running very slow.&lt;/p&gt;

&lt;h4 id=&quot;2-corrupt-data-node&quot;&gt;2. Corrupt data node&lt;/h4&gt;
&lt;p&gt;Each data node sends block report using which name node can identify if block is corrupted then will replicate the block from known good replica and mark the corrupted one for delete.&lt;/p&gt;

&lt;h3 id=&quot;name-node-failure&quot;&gt;Name node failure&lt;/h3&gt;
&lt;p&gt;In case of failure of name node there will be no access to metadata. So in hadoop v1 it was single point of failure. In hadoop v2 it has secondary name node so it’s not single point of failure.
Important metadata files:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;fsimage: snapshot of file system at a particular time&lt;/li&gt;
  &lt;li&gt;edit logs: transaction logs that changes in hdfs file system&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Secondary name node is responsible for merging fsimage with edit logs and generating new fsimage, once merge is done it restes fsimage and edit logs to empty. This process is done every 30 sec.&lt;/p&gt;

&lt;p&gt;So, when primary name node fails, secondary can become active and then we have to introduce secondary name node.&lt;/p&gt;

&lt;h3 id=&quot;block-size&quot;&gt;Block size&lt;/h3&gt;
&lt;p&gt;In hadoop v2 default block size is 128MB, if it is increased then we might have less blocks to process then nodes so cluster might be under utilized, if block size is decreased then we might have a lot of blocks which will increse burden on name node to keep track of metadata and handling it.&lt;/p&gt;

&lt;h3 id=&quot;hadoop-ha&quot;&gt;Hadoop HA&lt;/h3&gt;
&lt;p&gt;High availibility is achieved by two name nodes primary and hot standby. If primary goes down then standby takes overs. It can be achieved using quorum journal manager.&lt;/p&gt;

&lt;p&gt;Three journal nodes are used and metadata is copied over from active name node and here secondary is called as standby name node and still does the same checkpointing work to combine fsimage snaphot with edit log reading from journal nodes. Standby keeps lock on jk(zookeeper) and always check if its locked or not, if not then takes lock and becomes active.&lt;/p&gt;

&lt;p&gt;Note: Three journal nodes are used to tolerate single machine failure. The system can tolerate at most (N-1) / 2 failures when running with N JournalNodes.&lt;/p&gt;</content><author><name>{&quot;avatar&quot;=&gt;&quot;/assets/images/bio-photo.jpg&quot;, &quot;bio&quot;=&gt;&quot;Senior Data Engineer&quot;, &quot;links&quot;=&gt;[{&quot;label&quot;=&gt;&quot;Website&quot;, &quot;icon&quot;=&gt;&quot;fas fa-fw fa-link&quot;, &quot;url&quot;=&gt;&quot;https://khuranabal.github.io/&quot;}, {&quot;label&quot;=&gt;&quot;Linkedin&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-linkedin&quot;, &quot;url&quot;=&gt;&quot;https://www.linkedin.com/in/balpreet-singh-654705114&quot;}, {&quot;label&quot;=&gt;&quot;GitHub&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-github&quot;, &quot;url&quot;=&gt;&quot;https://github.com/khuranabal?tab=repositories&quot;}]}</name></author><category term="blog" /><category term="hdfs" /><summary type="html">hdfs is hadoop distributed file system. Highly fault tolerant and is designed to deploy on low cost machines.</summary></entry><entry><title type="html">Build and deploy static website using jekyll and github pages</title><link href="http://localhost:4000/blog/setup-github-pages-with-jekyll/" rel="alternate" type="text/html" title="Build and deploy static website using jekyll and github pages" /><published>2022-01-04T16:00:00+01:00</published><updated>2022-01-04T16:00:00+01:00</updated><id>http://localhost:4000/blog/setup-github-pages-with-jekyll</id><content type="html" xml:base="http://localhost:4000/blog/setup-github-pages-with-jekyll/">&lt;p&gt;To host website we have different ways and for this blog we are focussing on a use case where we need to have a website for blogging, and we are using:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;jekyll: static site generator, theme used is minimal mistakes&lt;/li&gt;
  &lt;li&gt;github pages: static website hosting service&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;installation&quot;&gt;Installation&lt;/h3&gt;
&lt;p&gt;Following are required to be installed in order to build and test the static website locally.
Installation instructions &lt;a href=&quot;https://jekyllrb.com/docs/installation/&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;ruby&lt;/li&gt;
  &lt;li&gt;rubygems&lt;/li&gt;
  &lt;li&gt;gcc&lt;/li&gt;
  &lt;li&gt;make&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;prerequisite&quot;&gt;Prerequisite&lt;/h3&gt;
&lt;p&gt;Use template repo to get started, we are using minimal-mistakes theme of jeykell, feel free to use any other theme as per need. Download &lt;a href=&quot;https://github.com/mmistakes/mm-github-pages-starter&quot;&gt;repo&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Test if its working fine on local by executing below command and then browsing on localhost:4000&lt;/p&gt;
&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bundle &lt;span class=&quot;nb&quot;&gt;install
&lt;/span&gt;bundle &lt;span class=&quot;nb&quot;&gt;exec &lt;/span&gt;jekyll serve
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note: config/post can be modified/added as required, follow &lt;a href=&quot;https://mmistakes.github.io/minimal-mistakes/docs/configuration/&quot;&gt;docs&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;configure-pages-and-deploy-to-github&quot;&gt;Configure pages and deploy to github&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;create new repo and configure github settings to enable pages, follow &lt;a href=&quot;https://docs.github.com/en/pages/quickstart&quot;&gt;here&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;push code to the new created repo&lt;/li&gt;
  &lt;li&gt;access githubpages url, if the repo created is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;username.github.io&lt;/code&gt; then url will be &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;https:\\username.github.io&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;enabling-feature-to-add-comments-in-the-blogs&quot;&gt;Enabling feature to add comments in the blogs&lt;/h3&gt;

&lt;p&gt;In order to enable comments feature we have different options and here we have used github issues where all the comments in the blog will be stored.
To enable, follow &lt;a href=&quot;https://mmistakes.github.io/minimal-mistakes/docs/configuration/#utterances-comments&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Note: It will not show comments locally, it will only work once pushed to github on githubpages url.&lt;/p&gt;</content><author><name>{&quot;avatar&quot;=&gt;&quot;/assets/images/bio-photo.jpg&quot;, &quot;bio&quot;=&gt;&quot;Senior Data Engineer&quot;, &quot;links&quot;=&gt;[{&quot;label&quot;=&gt;&quot;Website&quot;, &quot;icon&quot;=&gt;&quot;fas fa-fw fa-link&quot;, &quot;url&quot;=&gt;&quot;https://khuranabal.github.io/&quot;}, {&quot;label&quot;=&gt;&quot;Linkedin&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-linkedin&quot;, &quot;url&quot;=&gt;&quot;https://www.linkedin.com/in/balpreet-singh-654705114&quot;}, {&quot;label&quot;=&gt;&quot;GitHub&quot;, &quot;icon&quot;=&gt;&quot;fab fa-fw fa-github&quot;, &quot;url&quot;=&gt;&quot;https://github.com/khuranabal?tab=repositories&quot;}]}</name></author><category term="blog" /><category term="jekyll" /><category term="github" /><summary type="html">To host website we have different ways and for this blog we are focussing on a use case where we need to have a website for blogging, and we are using: jekyll: static site generator, theme used is minimal mistakes github pages: static website hosting service</summary></entry></feed>